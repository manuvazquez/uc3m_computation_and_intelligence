{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73803b6e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "> Clasificación de imágenes\n",
    "\n",
    "La clasificación de imágenes es una aplicación muy importante en aprendizaje automático: es lo que permite a los ordenadores \"ver\". Desde reconocer dígitos escritos a mano hasta identificar animales en fotos, el objetivo es enseñar a un modelo a asignar etiquetas a las imágenes basándose en lo que aprende de los ejemplos. En este *notebook*, construirás y probarás un clasificador de imágenes simple usando un conjunto de datos pequeño, explorando cómo las máquinas pueden aprender a distinguir entre dos tipos diferentes de objetos (¡¡por ejemplo, muffins y perros!!).\n",
    "\n",
    "# Preparación del entorno\n",
    "\n",
    "Todos los `import`s necesarios van aquí"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb36087",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import zipfile\n",
    "\n",
    "import fastai\n",
    "from fastai.vision.all import *\n",
    "from fastai.torch_core import set_seed\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edd6aef",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "En principio, podrías ejecutar el *notebook* tanto en *Colab* como localmente. ¿Se está ejecutando el *notebook* en *Colab*?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633e9bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    running_in_colab = True\n",
    "except ImportError:\n",
    "    running_in_colab = False\n",
    "\n",
    "running_in_colab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43e4b25",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "Si no estás ejecutando en *Colab*, puede que quieras elegir una GPU si hay varias disponibles. Ignora esto si estás ejecutando en *Colab*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be2dd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not running_in_colab:\n",
    "\n",
    "    import os\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cc4f4e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "¿Está disponible la aceleración por GPU?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d231bd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5f7a87",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "# Datos\n",
    "\n",
    "Vamos a usar el dataset [Muffin vs chihuahua](https://www.kaggle.com/datasets/samuelcortinhas/muffin-vs-chihuahua-image-classification/data) de [Kaggle](https://www.kaggle.com/https://www.kaggle.com/). Descargamos el archivo *.zip* correspondiente y lo descomprimimos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7480ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data will live inside this inside directory\n",
    "data_dir = pathlib.Path('muffin_chihuahua')\n",
    "\n",
    "# it it doesn't exist (from a previous run)...\n",
    "if not data_dir.exists():\n",
    "\n",
    "    # ...it is created\n",
    "    data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    # data is downloaded as a zip file, that will be named\n",
    "    zip_file = data_dir / 'muffin-vs-chihuahua-image-classification.zip'\n",
    "\n",
    "    # actual download\n",
    "    !curl -L -o {zip_file} https://www.kaggle.com/api/v1/datasets/download/samuelcortinhas/muffin-vs-chihuahua-image-classification\n",
    "\n",
    "    # data is unzipped inside the data directory\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zf:\n",
    "        zf.extractall(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524b8c63",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "¿Qué hay en el directorio?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f681e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(data_dir.iterdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3f218c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "Por lo tanto, además del *.zip* que acabamos de descargar, tenemos los conjuntos habituales *train* y *test*, y dentro de cada uno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7540c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in ['train', 'test']:\n",
    "    print(f\"{e}:\")\n",
    "    for c in ['chihuahua', 'muffin']:\n",
    "        n = len(list((data_dir / e / c).iterdir()))\n",
    "        print(f\"  {c}: {n}\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344cf841",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "Nos centraremos solo en los datos de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0029d184",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = data_dir / 'train'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124933e5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "## Carga de datos\n",
    "\n",
    "Utilizaremos dos librerías de *deep learning* muy conocidas: [fastai](https://github.com/fastai/fastai) y [PyTorch](https://pytorch.org/).\n",
    "\n",
    "Vamos a dejar que [fastai](https://github.com/fastai/fastai) lea las imágenes de las distintas carpetas (teniendo en cuenta las etiquetas) y cree automáticamente una división *train*/*validation*. Lo que hace el código de abajo, en resumen, es:\n",
    "\n",
    "- Definir la(s) transformación(es) que se aplicarán a cada *item* individual (es decir, imagen). Redimensionaremos cada imagen a un tamaño manejable (por ejemplo, 256×256).\n",
    "\n",
    "- Definir la(s) transformación(es) que se aplicarán a cada *batch*. Estas incluyen (importante!!) *augmentations* (consulta la [documentación](https://docs.fast.ai/vision.augment.html#aug_transforms) para ver qué es posible).\n",
    "\n",
    "- Instanciar un objeto `DataBlock`, que es una especie de \"plantilla\" que especifica cómo acceder al dataset.\n",
    "\n",
    "- Usar el `DataBlock` para obtener los `DataLoader`s (objetos de *PyTorch* pensados para recorrer el cojunto de datos batch a batch), uno para el conjunto de *train* y otro para el de *validation*.\n",
    "\n",
    "Observa que se aplican dos operaciones de *resize*: una en `item_tfms` y otra en `batch_tfms`. No te preocupes por los detalles, pero la última es la que realmente determina el tamaño de las imágenes con las que se entrenará el modelo, y la primera es solo un paso de *predimensionado* que ayuda a evitar artefactos en las imágenes transformadas (no es necesario, pero se puede encontrar una explicación completa en el [libro de fastai](https://nbviewer.org/github/fastai/fastbook/blob/master/05_pet_breeds.ipynb#Presizing))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf807c9",
   "metadata": {
    "id": "dls"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "# a transformation to apply to every item\n",
    "item_tfms = [Resize(256)]\n",
    "\n",
    "# a `list` of transformations to apply to every *batch*\n",
    "batch_tfms = [*aug_transforms(size=224, max_warp=0), Normalize.from_stats(*imagenet_stats)]\n",
    "\n",
    "dblock = DataBlock(\n",
    "    blocks=(ImageBlock, CategoryBlock),\n",
    "    get_items=get_image_files,\n",
    "    get_y=parent_label,\n",
    "    splitter=RandomSplitter(valid_pct=0.2, seed=42),  # TODO: try a different seed\n",
    "    item_tfms=item_tfms,\n",
    "    batch_tfms=batch_tfms\n",
    ")\n",
    "\n",
    "dls = dblock.dataloaders(data_root, bs=batch_size)\n",
    "dls.show_batch(max_n=12, figsize=(8,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcd86f4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "Entre las transformaciones de *batch* (las pasadas mediante el parámetro `batch_tfms`) tenemos la función `aug_transforms`. Esta realiza [data augmentation](https://en.wikipedia.org/wiki/Data_augmentation). La idea es: una foto de un perro sigue siendo una foto de un perro si la rotas un poco...o la deformas un poco...o la recortas un poco (por ejemplo, te quedas solo con la cabeza). Entonces, cada una de estas *versiones* de la misma imagen se puede usar en el entrenamiento del modelo, y estamos generando (inventando) datos de entrenamiento de forma *artificial* (¡¡una cantidad infinita!!...ya que la cantidad de rotación/deformación/lo-que-sea se elegirá aleatoriamente en cada iteración del procedimiento de entrenamiento). *Data augmentation* es un truco muy común en visión por ordenador porque es muy útil (y computacionalmente barato).\n",
    "\n",
    "<font color='red'>TO-DO</font>: Observa que hay una fuente adicional (además de la implícita en el uso de *data augmentation*) de *aleatoriedad* aquí: el parámetro `seed` pasado a `RandomSplitter`. ¿Para qué sirve?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47df4b90",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "# Entrenamiento del modelo\n",
    "\n",
    "Vamos a hacer [transfer learning](https://en.wikipedia.org/wiki/Transfer_learning) explotando un modelo pre-entrenado como *mobilenet_v3_small*. Más sobre esto en cursos futuros...pero *transfer learning* nos permite reutilizar un modelo entrenado para una tarea en otra tarea diferente (pero relacionada). En este caso, estamos aprovechando un modelo entrenado para clasificar imágenes en [imagenet](https://www.image-net.org/)... que tiene 1.000 categorías.\n",
    "\n",
    "La *métrica* que nos interesa es la precisión (*accuracy*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b9b4fd",
   "metadata": {
    "id": "train"
   },
   "outputs": [],
   "source": [
    "learn = vision_learner(dls, mobilenet_v3_small, metrics=accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba37a0f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "Vamos a entrenar el modelo durante unas pocas *epochs*. Cada *epoch* recorre el dataset completo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cc53af",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fine_tune(\n",
    "    epochs=3,\n",
    "    base_lr=3e-3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b94620",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "<font color='red'>TO-DO</font>: ¿Qué porcentaje de las veces se equivoca el modelo?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9c44ba",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "<font color='red'>TO-DO</font>: ¿Qué pasa si vuelves a entrenar el modelo (ejecutando otra vez el último par de celdas)? ¿Obtienes los mismos resultados? ¿Por qué no? (*Pista*: ¿cómo se inicializan los parámetros del modelo cada vez que llamas a `vision_learner`?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdf8970",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "# Resultados\n",
    "\n",
    "Veamos algunas predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cc599c",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.show_results(max_n=9, figsize=(8,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad93dc3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "Aunque el rendimiento es muy bueno, es interesante ver dónde el modelo tiene más dificultades.\n",
    "Vamos a mirar la [matriz de confusión](https://es.wikipedia.org/wiki/Matriz_de_confusi%C3%B3n) para ver si al modelo se equivoca más con una clase que con otra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5abcf44",
   "metadata": {
    "id": "eval"
   },
   "outputs": [],
   "source": [
    "interp = ClassificationInterpretation.from_learner(learn)\n",
    "interp.plot_confusion_matrix(figsize=(4,4), dpi=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4772b4cc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "<font color='red'>TO-DO</font>: A la vista de esta matriz de confusión, ¿el rendimiento del modelo depende de si la entrada es de una clase u otra?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed90a652",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "Veamos algunas de las imágenes con las que el modelo tuvo más problemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a5b1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "interp.plot_top_losses(4, nrows=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4abf543",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "<font color='red'>TO-DO</font>: ¿Podrías adivinar la clase por tu cuenta?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c5fd06",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "# El modelo con tus propias imágenes\n",
    "\n",
    "Elige una foto (muffin, chihuahua...u otra cosa) y pasásela al model para ver que predice. **La GUI requiere *Colab***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37aacc2",
   "metadata": {
    "id": "predict"
   },
   "outputs": [],
   "source": [
    "if running_in_colab:\n",
    "\n",
    "    try:\n",
    "        from google.colab import files\n",
    "        up = files.upload()\n",
    "        for fn in up:\n",
    "            img = PILImage.create(fn)\n",
    "            pred,pred_idx,probs = learn.predict(img)\n",
    "            print(f'{fn} -> {pred}; probs={probs.tolist()}')\n",
    "            display(img.to_thumb(256,256))\n",
    "    except Exception as e:\n",
    "        print('Local environment or no file uploaded:', e)\n",
    "\n",
    "else:\n",
    "\n",
    "    print(\"The GUI requires Colab.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c8ca6c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "La probabilidad que obtienes es la de la primera etiqueta, es decir, la de un *chihuahua*.\n",
    "\n",
    "<font color='red'>TO-DO</font>: ¿Qué pasa si le das al modelo algo que no es ni un muffin ni un chihuahua?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a58027c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "# Ajuste de hiperparámetros\n",
    "\n",
    "## Augmentation\n",
    "\n",
    "Vamos a experimentar con *data augmentation*...\n",
    "\n",
    "<font color='red'>TO-DO</font>: Explora otros tipos de *augmentation* pasando diferentes parámetros a la función `aug_transforms`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbff19c1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# aug = aug_transforms(..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1670cea",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "Necesitamos crear un nuevo `DataBlock` (diferente), que se puede construir modificando el anterior. Se obtienen `DataLoaders` actualizados a partir de él."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04cdf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_dblock = dblock.new(item_tfms=item_tfms,batch_tfms=[*aug, Normalize.from_stats(*imagenet_stats)])\n",
    "aug_dls = aug_dblock.dataloaders(data_root, bs=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a5199a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "Vamos a visualizar un batch con las nuevas *augmentations* para ver es aspecto que tienen los datos \"nuevos\"..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a50b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_dls.show_batch(max_n=12, figsize=(8,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb960b5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "...antes de entrenar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db3f0a8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "aug_learn = vision_learner(aug_dls, mobilenet_v3_small, metrics=accuracy)\n",
    "aug_learn.fine_tune(epochs=2, base_lr=3e-3)  # quick test\n",
    "print('Accuracy:', aug_learn.validate()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3dd8a54",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "<font color='red'>TO-DO</font>: ¿Obtienes mejores resultados?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d86f0c5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "## Arquitectura diferente\n",
    "\n",
    "<font color='red'>TO-DO</font>: Prueba un modelo *más grande*, como uno de la familia *resnet* (por ejemplo, `resnet50`). Puedes quedarte con el `DataBlock` original. ¿Vale la pena, considerando el aumento en el tiempo de entrenamiento?\n",
    "\n",
    "Debería ser posible obtener una lista incompleta de modelos disponibles usando\n",
    "```\n",
    "import timm\n",
    "timm.list_models()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7511e7",
   "metadata": {},
   "source": [
    "# Sample questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253b7615",
   "metadata": {},
   "source": [
    "## What is the main goal of the image classification model described here?\n",
    "- [ ] To make image files smaller so they fit on disk\n",
    "- [ ] To decide which of two labels best matches a picture (for example, which kind of object it shows)\n",
    "- [ ] To turn all color pictures into black-and-white\n",
    "- [ ] To draw new pictures from scratch\n",
    "\n",
    "## Why are the images split into a training set and a validation set?\n",
    "- [ ] So the model can be trained on one part and then checked on images it has not seen\n",
    "- [ ] So that each image gets two different labels\n",
    "- [ ] So that half of the images can be safely deleted\n",
    "- [ ] So that the images can be sorted by file name"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
