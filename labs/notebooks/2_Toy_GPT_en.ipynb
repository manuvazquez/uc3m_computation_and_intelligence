{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "> Toy ChatGPT\n",
    "\n",
    "Imagine building a tiny *ChatGPT* of your own (small enough to understand in one sitting, but real enough to see the magic of text prediction come alive). In this notebook we'll follow [Andrej Karpathy](https://karpathy.ai/)'s approach: write the code from scratch, keep it simple, and train a little model that can learn to generate text one character at a time. We're not aiming for power or speed; the goal is to demystify how these models actually work under the hood. By the end, you'll have a hands-on sense of how a GPT-like system can be built step by step, and you'll be able to play with it, experiment, and make it your own.\n",
    "\n",
    "This is based on Karpathy's [ng-video-lecture](https://github.com/karpathy/ng-video-lecture/tree/master) and [minGPT](https://github.com/karpathy/minGPT) repositories, and the [companion video tutorial](https://www.youtube.com/watch?v=kCc8FmEb1nY).\n",
    "\n",
    "# Setup\n",
    "\n",
    "In principle, you could run the notebook either in *Colab* or locally. Is the notebook running in *Colab*?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    running_in_colab = True\n",
    "except ImportError:\n",
    "    running_in_colab = False\n",
    "\n",
    "running_in_colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "## GPU\n",
    "\n",
    "In order to run this notebook (in a reasonable time) we will make use of the [Grahics Processing Unit](https://en.wikipedia.org/wiki/Graphics_processing_unit) (GPU) provided by the *Colab* environment. To enable it, on the top righ-hand-side of the *Colab* interface, click `Conectar`, `Cambiar tipo de entorno de ejecución`, select `GPU T4`, and then click `Guardar`. `to_device` calls strewn all over the notebook are meant to *move* arrays to the GPU (if available).\n",
    "\n",
    "If not running in *Colab*, you might want to choose a GPU if several are available. Ignore, if you are running in *Colab*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not running_in_colab:\n",
    "\n",
    "    import os\n",
    "    os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "## Python libraries\n",
    "\n",
    "Required `import`s are centralized here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "Is there a GPU available?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "# Data curation\n",
    "\n",
    "Let us download some text. The code below will download Shakespeare's texts, but you can essentially plug in here any text resource (a book, some webpage...) you like!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "id": "h5hjCcLDr2WC",
    "outputId": "43f10117-d670-438f-fc0b-c6a0b7d64a3a",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "The *whole* file is read into memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se lee en memoria el archivo *completo*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "O6medjfRsLD9",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "print('number of characters read: ', len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "Let us take a look at our dataset.\n",
    "\n",
    "<font color='red'>TO-DO</font>: Show the first 200 characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "## Vocabulary\n",
    "\n",
    "As usual, in order to turn text into numbers we need a **vocabulary** allowing to assemble, piece by piece, the whole dataset. For the sake of simplicity, let that be individual characters that show up in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "id": "0e-Rbyr8sfM8",
    "outputId": "5e2a5946-9664-4c0a-cbe8-236ca8b28d36",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "chars = sorted(set(text))\n",
    "vocab_size = len(chars)\n",
    "print(rf'Vocabulary ({len(chars)} elements) is: {''.join(chars)} ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "We will associate an index (`i`) with every element (string, `s`) in the vocabulary. Any mapping is fine, so the easy thing is associate every character with its index in the `list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yw1LKNCgwjj1",
    "outputId": "c02c56da-160a-41ef-8c16-819e18d5aec8"
   },
   "outputs": [],
   "source": [
    "stoi = { ch:i for i, ch in enumerate(chars) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "(`stoi` as in \"string to integer\")\n",
    "\n",
    "You can use it to find out the index of any character you like, e.g.,\n",
    "\n",
    "<font color='red'>TO-DO</font>: What is the index of character `k`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "We would also like the inverse mapping, i.e., from index to character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itos = chars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "<font color='red'>TO-DO</font>: What is the character associated with index `12`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "We exploit the above mappings (`dict`) to make functions able to operate, respectively, on **sequences** of characters (for *encoding*)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "print(encode(\"hii there\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "...and numbers (for *decoding*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "print(decode(encode(\"hii there\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "Let us `encode` (characters to numbers) the dataset into a *torch* `Tensor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YJb0OXPwzvqg",
    "outputId": "673e43ab-31a5-4758-defa-b1198285ab27"
   },
   "outputs": [],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "It has the same number of elements as `text` above, and every one of the elements is a 64-bits integer (`torch.int64`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(text) == len(data)\n",
    "print(data.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "Let us print the first characters, now represented as numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "## Training/validation split\n",
    "\n",
    "Data are split into *training* and *validation* sets, so that we have a way of knowing how well the model is generalizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f_WIXqxz0lU5"
   },
   "outputs": [],
   "source": [
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "## Block size\n",
    "\n",
    "Since we cannot process *all* the data at once (unless you have a very small dataset, which would probably end up producing a crappy model), we need to *chunk* it. Let us consider chunks of size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TD5Bj8Y6IAD4",
    "outputId": "738485b8-30a8-4d15-d265-39ca4ba7d509"
   },
   "outputs": [],
   "source": [
    "block_size = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "[GPT-4](https://en.wikipedia.org/wiki/GPT-4), for instance, has a block size (aka, *context length*) of tens of thousands of *tokens*, every one of them encompassing more than one character. Hence, keep in mind that we are, of course, looking at a toy example\n",
    "\n",
    "Let us look at the first block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[:block_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "When processing every block the goal is to predict a character given *all* the previous ones: for predicting the 2nd character we will only use the 1st one, when predicting the 3rd characer, we'll use the 1st and 2nd...and so on and so forth. In principle, this would mean that, for every block size, we would be making `block_size - 1` predictions. An extra character, the `(block_size+1)`-th character, is always considered so that we have exactly `block_size` predictions. Hence, the above block will yield the following prediction tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9HXDe8vGJCEn",
    "outputId": "66670795-362e-48ec-f681-1e2d3b783fd5"
   },
   "outputs": [],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f'When input is {context.tolist()}, the target: {target}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "english"
    ]
   },
   "source": [
    "Ultimately, this is a *multiclass* classification problem: each prediction is not just a single label, but a **full probability distribution** over all possible classes. In our case, it reflects how likely each character in the vocabulary is to be the next one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "## Batching\n",
    "Since we want to get the most of GPUs (parallel processing), we will actually pack together and process several blocks at the same time...as many as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "Let us fix a pseudo-random numbers generator (PRNG) *seed* so that we get always the same results (when requesting \"random\" numbers below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q3k1Czf7LuA9",
    "outputId": "e8a938ff-19d3-4ff2-fa1b-4bd2b6a7a1af"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "An auxiliary function to assemble a random batch, either from the *training* or from the *validation* set (depending of the value of the `split` parameter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split: str):\n",
    "    \n",
    "    # either the training or validation set\n",
    "    data = train_data if split == 'train' else val_data\n",
    "\n",
    "    # a random index (for each block in the batch) that is followed by at least `block_size` characters so that we can extract a full block\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    \n",
    "    # notice the `stack`ing\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "Let us request a batch from the *training* set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb, yb = get_batch('train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "...and take at look at inputs and outputs.\n",
    "\n",
    "- **Input** and its dimensions (`bath_size`, `block_size`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xb)\n",
    "print(xb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "- **Target** (output) and its dimensions (`bath_size`, `block_size`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(yb)\n",
    "print(yb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "<font color='red'>TO-DO</font>: show the *text* represented by `yb` (the outputs or targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "Notice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb.shape == yb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "The above batch poses the following prediction problems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for every sequence in the batch...\n",
    "for i_b, b in enumerate(range(batch_size)):\n",
    "\n",
    "    print(f'{i_b}-th element in the batch:')\n",
    "    \n",
    "    # for every element in the sequence...\n",
    "    for t in range(block_size):\n",
    "        \n",
    "        # every character in the sequence up to and including (hence the `+1`) t\n",
    "        context = xb[b, :t+1]\n",
    "        \n",
    "        # by construction (above), `yb[b,t]` is the target for the sequence up to and including t\n",
    "        target = yb[b,t]\n",
    "        \n",
    "        print(f\"When input is {context.tolist()}, the target: {target}\")\n",
    "\n",
    "    print('-'*5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "What must go into the neural network (NN) are actually `tensor`s and **not** `list`s of *variable* size. The input to the NN will be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qpyyAeIzQjlO",
    "outputId": "59e7424f-1267-4ecd-bd24-c68d52291bda"
   },
   "outputs": [],
   "source": [
    "xb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "and the corresponding output (*target*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "That yields `bath_size` $\\times$ `block_size` (the dimesions of `xb` and `yb`) *independent* predictions for the model to learn ([Karpathy's explanation](https://youtu.be/kCc8FmEb1nY?t=1281)). They are all processed simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "ZcvKeBXoZFOY",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "# Training\n",
    "\n",
    "## Parameters\n",
    "\n",
    "Let us set some (hyper)parameters we will actually be using for training the model\n",
    "\n",
    "- Seen above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hoelkOrFY8bN",
    "outputId": "13811e1e-29c7-4794-c8d3-0a783c607aaa"
   },
   "outputs": [],
   "source": [
    "block_size = 32\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "- How many (random) batches to train the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_iters = 5000\n",
    "max_iters = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "- Some architecture-specific parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "* In connection with training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "## Model\n",
    "\n",
    "This is the definition of the NN (based on *transformers*). **Skip** for now: in this course we are not yet interested in the implementation details, even though, as you can see, the code is actually not so large. If you delve into the code (again, not required), keep in mind that this was written with an educational frame of mind, and there are some very questionable programming practices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "# Renamed from https://github.com/karpathy/ng-video-lecture/blob/52201428ed7b46804849dea0b3ccf0de9df1a5c3/bigram.py#L61\n",
    "class ToyLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "All the above mumbo-jumbo is just meant to define a (huge) function, instantiated as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ToyLanguageModel().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "english"
    ]
   },
   "source": [
    "Let us evaluate `model` at the above batch, `xb` (not built considering the parameters we are assuming now, but still OK)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yb_est = model(xb.to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "<font color='red'>TO-DO</font>: What do you get? Explain the dimensions of any `Tensor`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "## Training loop\n",
    "\n",
    "A function to estimate the *loss*, a measure of how well we are doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X.to(device), Y.to(device))\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "*Boilerplate* for training the model. This is already pretty understandable for you right now. In any case, just focus on knowing \"what's going on\" at a high level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ToyLanguageModel()\n",
    "\n",
    "m = model.to(device)\n",
    "\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb.to(device), yb.to(device))\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "<font color='red'>TO-DO</font>: Is it guaranteed that you have trained on the *whole* dataset (i.e., that every character has been used)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "<font color='red'>TO-DO</font>: Take a look at the dimensions of `logits` (from the last iteration in the training loop), and explain them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "In order to exploit the trained model to generate some new text, we need to set up a context (sort of a starting point)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "<font color='red'>TO-DO</font>: What is the text associated with the context?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "Let us generate some new text (up to 2,000 characters) using the above context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "# Experiments\n",
    "\n",
    "- <font color='red'>TO-DO</font>: Train the model for a shorter time (i.e., on a smaller number of batches, say 10), and try generating text. What do you observe? How does the `loss` at the end of traing compare against that of the first model you trained?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "- <font color='red'>TO-DO</font>: Try different block sizes. Can you get better results?\n",
    "\n",
    "- <font color='red'>TO-DO</font>: Try on a different, smaller, dataset (other than Shakespeare's). You could, for instance, *inject* the lyrics of your favorite song (that would be a *tiny* dataset) in the `text` variable up above. What do you observe? Compare the values of the loss for *training* and *validation*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "english"
    ]
   },
   "source": [
    "# Sample questions\n",
    "\n",
    "## What does the *batch size* control during training?\n",
    "- [ ] How many layers the model has  \n",
    "- [ ] The number of training epochs\n",
    "- [ ] How many small data chunks are processed in parallel before updating model parameters\n",
    "- [ ] The maximum number of characters generated\n",
    "\n",
    "---\n",
    "\n",
    "## What is the model trying to learn during training?\n",
    "- [ ] The meaning of words and sentences  \n",
    "- [ ] The grammatical rules of English  \n",
    "- [ ] The emotional tone of Shakespeare’s plays\n",
    "- [ ] The probability of the next character given the previous ones"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "cei",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
