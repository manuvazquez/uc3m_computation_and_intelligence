{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8480e489",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "> Introduction to Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a8c5cf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "Natural Language Processing (NLP) is all about teaching computers to understand and work with human language (the words we use every day). From chatbots and translation apps to spam filters and voice assistants, NLP powers many tools we rely on without even noticing. In this notebook, we’ll explore some of the basic techniques that let machines handle text in meaningful ways."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c5c347",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "## Setup\n",
    "\n",
    "Some `import`s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bd25cc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3967a7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "Below we'll be using two [nltk](https://www.nltk.org) resources that are not installed along with the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787ad19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c80d6d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "<font color='red'>TO-DO</font> What are the two NLTK resources we downloaded above?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bd7662",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "# Text Preprocessing\n",
    "\n",
    "Text is inherently *unstructured data* in the sense that it does not follow a fixed format (it's not a neat table, array or spreadsheet). Hence, some *preprocessing* is required before text can be fed into a machine learning (ML) model. Learning to do this *preprocessing* is the purpose of this section.\n",
    "\n",
    "Notice that\n",
    "> As machine learning algorithms process numbers rather than text, the text must be converted to numbers.\n",
    ">\n",
    "> -- <cite>[Wikipedia](https://en.wikipedia.org/wiki/Large_language_model#Tokenization)</cite>\n",
    "\n",
    "### Tokenization\n",
    "\n",
    "Tokenization is the process of splitting a text into smaller pieces, called tokens. These tokens can be words, phrases, or even sentences. Each token is then assigned a (unique) number. Tokenization is a crucial step in NLP as it allows us to analyze and process textual data more effectively.\n",
    "\n",
    "CAVEAT: This is markedly different from [*Lexical* tokenization](https://en.wikipedia.org/wiki/Lexical_analysis#Lexical_token_and_lexical_tokenization), the latter being of no interest to us in this course.\n",
    "\n",
    "Let's *tokenize* the following text (you can replace it with any text you like)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22efb239",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'They can kill you, but the legalities of eating you are quite a bit dicier (from \"Infinite Jest\", by David Foster Wallace)'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5d4f04",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "using the `nltk` library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5515baa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(text)\n",
    "print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bd0bbb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "With this particular *tokenizer* each token is a word, and punctuation marks are treated as separate tokens. This is useful for many NLP tasks, but there are other tokenizers that can handle punctuation differently or tokenize at the character level. \n",
    "\n",
    "After tokenization, you just number the tokens so each one will have a number associated with it. That will be its index in the list of possible tokens that make up the **vocabulary** of the model, i.e., the set of possible tokens.\n",
    "\n",
    "<font color='red'>TO-DO</font> What would be here the vocabulary?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0b26af",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "### Stopwords Removal\n",
    "\n",
    "Clearly some words are more informative than others. If a toddler says \"cat tree\", you can guess she's probably trying to say \"there is cat on the tree\". Words such as \"the\" or \"a\" are very common in English, but they don't usually carry much meaning on their own. These common words are called **stopwords**. Removing stopwords can help reduce *noise* in the data and improve the performance of NLP models.\n",
    "\n",
    "There are predefined *lists of stopwords* to be exploited (we downloaded one of such lists above!!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcf3e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stop_words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e39ce8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "We could exploit them to *filter* the above list of `tokens`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56adb702",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_tokens = list(filter(lambda w: w.lower() not in stop_words, tokens))\n",
    "filtered_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11427ef",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "<font color='red'>TO-DO</font>: Which words were removed as stopwords?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f944ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(tokens) - set(filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5421bdd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "Later on we will look at an example showing the effect of removing stopwords.\n",
    "\n",
    "# Feature Extraction: Bag‑of‑Words & tf-idf\n",
    "\n",
    "Instead of NLTK, here we will make use of [scikit-learn](https://scikit-learn.org), a popular (sort of *standard*) library for ML. It exhibits a number of tools for text processing, including feature extraction methods such as Bag-of-Words and tf-idf (stay tuned!!).\n",
    "\n",
    "Let us make up some *documents*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6210e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    \"I love programming in Python.\",\n",
    "    \"Python is great for NLP tasks.\",\n",
    "    \"I enjoy learning new languages.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4840e5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "<!-- First step is to come up with a **dictionary** and count the number of times every one of its elements shows up in every *document*.  This is a  -->\n",
    "How can we turn each document into a *fixed-size* vector of numbers? Once we have a dictionary, we can simply count the number of times every one of its elements shows up in every *document*. Then, every document is represented by a vector the size of the vocabulary. This *transformation* is a really\n",
    "common task that *scikit-learn* automates through the class [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html). What you obtain is a [Bag-of-words](https://en.wikipedia.org/wiki/Bag-of-words_model) (or *BoW*) representation of the corpus (in which every document becomes a vector the size of the vocabulary).\n",
    "\n",
    "<font color='red'>TO-DO</font>: Exploit the class [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)  (it supports *stopwords*) to *vectorize* the above `list` of *documents*. Print the vector, i.e., the *counts*, for the first *document*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed92566",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "<font color='red'>TO-DO</font>: What's the vocabulary? How many times does the 3rd word in the vocabulary show up in the 2nd document?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac791df1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "We now have a way of representing every text document in a corpus as a fixed-size vector of numbers (*counts*). However, even after getting rid of *stopwords*, which have little-to-zero meaning, it's clear that not every word is equally significant. Intuitively, words that show up *all the time* in a corpus (e.g., the word \"medical\" in a bunch of documents on medicine) are not very significant and, viceversa, words that occur very little might yield some useful hints for a task. We have a name for this intuition: [term frequency–inverse document frequency](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) or *tf-idf*. The idea is: for each document, we count the relative number of times (*frequency*) each word (*term*) shows up and divide it by the relative number of times it does in the corpus (*document frequency*).\n",
    "\n",
    "<font color='red'>TO-DO</font>:  Use the [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) class to obtain the tf-idf instead of *raw* counts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4bdd17",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "<font color='red'>TO-DO</font>: What is the word with the smallest non-zero tf-idf values across all documents?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2340cb4f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "<font color='red'>TO-DO</font>:\n",
    "\n",
    "- How is *sparsity* (every document only includes a small number of terms in the vocabulary) handled by the above classes (to avoid taking up a huge amount of memory)?\n",
    "- After adding a document **without** the word “python,” should the tf-idf of the latter increase or decrease in the original documents?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22800b3c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "## Why Stopwords Matter\n",
    "\n",
    "Notice that, once every document becomes a fixed-size (that of the vocabulary, say $N$) vector of numbers, it's easy to compare documents by comparing their corresponding vectors. In principle, any *distance* in $\\mathbb{R}^N$ is amenable to be used.\n",
    "\n",
    "We'll compute [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity) between two short sentences **with** and **without** stopword removal to illustrate how stopwords can inflate similarity scores. A slightly contrived corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9f69e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_sentence = \"The cat sat on the mat\"\n",
    "another_sentence = \"A dog sat on the rug\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0199eaef",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "Are the sentences similar?\n",
    "\n",
    "**Without** removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a54149",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_all = CountVectorizer().fit_transform([a_sentence, another_sentence])\n",
    "print(\"Cosine similarity (without stopwords):\",\n",
    "      cosine_similarity(vec_all)[0,1].round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e75e47",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "**After** removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86f5029",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_ns = CountVectorizer(stop_words='english').fit_transform([a_sentence, another_sentence])\n",
    "print(\"Cosine similarity (with stopwords):\",\n",
    "      cosine_similarity(vec_ns)[0,1].round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c655db",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "Notice how removing stopwords lowers the similarity by excluding common words like *the* and *on*, giving a truer sense of semantic distance.\n",
    "\n",
    "# Text Classification with Naive Bayes\n",
    "\n",
    "Let us put to use what we have learned in building a *news classifier*: given a piece of news, the task is to decide the *category* (theme, topic) among a set of competing ones. We'll be making use of **2 categories**,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c1a57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['rec.autos', 'rec.sport.baseball']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4093e0e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    ", of the `20 Newsgroups` dataset, which is readily available through *scikit-learn*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de28541b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = fetch_20newsgroups(subset='train', categories=categories, remove=('headers','footers','quotes'))\n",
    "test  = fetch_20newsgroups(subset='test',  categories=categories, remove=('headers','footers','quotes'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162f3ed1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "<font color='red'>TO-DO</font>: Take a look at a couple of *posts* (either from the *training* or *test* set), along with their corresponding *category* (i.e., label)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bb28d2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "<font color='red'>TO-DO</font>: Turn the documents into tf-idf *counts*. For that, `fit` the *vectorizer* on the training set and, afterwards, use the resulting `TfidfVectorizer` object to `transform` both the training and test sets separately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c13b669",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "<font color='red'>TO-DO</font>: Take a look at the vector of numbers for the previously checked *posts*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e31afaf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "<font color='red'>TO-DO</font>: What happens when there is a word in the test set that was not seen in the training set (when you `fit_transform` on the latter)? Make up a document with a single non-existent word and look how many non-zero elements are in its tf-idf representation (obtained through the `*Vectorizer` fitted on the training set)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6856cce2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "Let us train a simple [Naive Bayes classifier](https://en.wikipedia.org/wiki/Naive_Bayes_classifier) on the *training set*..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb54ca9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, train.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49cb472",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "...and use it to predict on the *test set*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b59f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eca878d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "<font color='red'>TO-DO</font>: What is the overall accuracy of the classifier? You need to compare the above `pred`ictions against the actual `target`s in the *test* set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb42a79a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "<font color='red'>TO-DO</font>: Check one of the missclasified *posts*. Are you able to tell which class it belongs to (i.e., what it is about)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292c34a3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "<font color='red'>TO-DO</font>: How come all the texts end up with the same length?\n",
    "\n",
    "# Introduction to Topic Modeling\n",
    "\n",
    "Topic modeling uncovers latent themes in a corpus *without supervision* (notice that above we had *labels*, i.e., superivision, for each piece of news). We’ll use [Latent Dirichlet allocation](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation) or LDA (not to be confused with *Linear discriminant analysis*, also abbreviated in the literature as LDA).\n",
    "\n",
    "Let us get more data from the `20 Newsgroups` dataset...but this time without making use of the labels (i.e., the categories). Indeed, we are only using the `data` attribute (**not** the `target`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be21e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['talk.politics.misc', 'comp.graphics', 'sci.space']\n",
    "data = fetch_20newsgroups(subset='train', categories=categories,remove=('headers','footers','quotes'))\n",
    "docs = data.data  # list of raw text documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45235ea",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "Let us first *vectorize* the copus using a vanilla `CountVectorizer`, the latter being more aligned with assumptions behind LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e092f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer(stop_words='english')\n",
    "X = vec.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1f9e48",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "Let us `fit` the model using LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f7a9a4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components=3, random_state=42)\n",
    "lda.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49882c7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "Let us take a look at the most important (most likely) words in every topic (aka, `component`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe543fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = vec.get_feature_names_out()\n",
    "for idx, topic in enumerate(lda.components_):\n",
    "    top_terms = [vocab[i] for i in topic.argsort()[-5:][::-1]]\n",
    "    print(f\"Topic {idx+1}: {top_terms}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2174b48",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "Can you match the above topics with *categories* in the `20 Newsgroups` dataset?\n",
    "\n",
    "<font color='red'>TO-DO</font>: It might be convenient to ignore some of the words in the above topics. Re-*fit* the model excluding a couple of words: \"mr\" and \"don\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70a0790",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "<font color='red'>TO-DO</font>: What happens when you increase or decrease the number of topics? Notice that, right now, we now have more topics than *actual* categories in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dddf8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_topics(docs, n_components=2, stop_words=list(ENGLISH_STOP_WORDS) + ['mr', 'don'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7a28e0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "<font color='red'>TO-DO</font>: Can a word show up in two different topics? What happens when you modify the value of `random_state` passed to `LatentDirichletAllocation`? Why is that?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cei",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
