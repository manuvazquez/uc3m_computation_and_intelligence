{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77e00c05",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "> Generative modeling\n",
    "\n",
    "In this notebook, we’ll use a [Variational Autoencoder](https://en.wikipedia.org/wiki/Variational_autoencoder) (VAE) as a kind of “artist’s shorthand” for images of faces. The encoder learns to compress each image into just a few *latent* variables (a rough sketch that keeps the main traits but not every tiny detail) while the decoder learns to turn that sketch back into a full image. We also gently force this \"compressed space\" to follow a simple, standard [Gaussian](https://en.wikipedia.org/wiki/Normal_distribution) (a nice, round “cloud” of points), so we know what kind of latent variables are valid. That’s the fun part: because this space is smooth and well-behaved, nearby points correspond to similar faces, and we can safely sample random points from this Gaussian cloud to generate new faces the model has never seen before. The picture below tries to convey the main idea (notice that, on the left we have a 3D space, whereas the one on the right is 2D).\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"https://raw.githubusercontent.com/manuvazquez/uc3m_computation_and_intelligence/master/labs/notebooks/figures/vae.svg\" alt=\"Description\" width=\"1200\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7435f03",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "# Setup\n",
    "\n",
    "In principle, you could run the notebook either in *Colab* or locally. Is the notebook running in *Colab*?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b53fe99",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    running_in_colab = True\n",
    "except ImportError:\n",
    "    running_in_colab = False\n",
    "\n",
    "running_in_colab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c09355",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "If running in *Colab* we must install a couple of Python libraries. If not, you might want to choose a GPU if several are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730d9c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if running_in_colab:\n",
    "    !pip install equinox numpyro\n",
    "else:\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e064d82e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "The remaining required `import`s come here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b44002f-d6e8-4a96-8066-f755b50de837",
   "metadata": {
    "id": "6eQChN-ouFDl"
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset\n",
    "from torch import optim\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "import equinox as eqx\n",
    "import optax\n",
    "from jaxtyping import Array, Float, Int\n",
    "import numpyro.distributions as dist\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import kagglehub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66b17ca",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "For the sake of reproducibility, we set random *seeds*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2b441c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3c7397",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "Device to be used...you should see *[CudaDevice(id=0)]* or similar (the *Cuda* prefix being the important bit) if you intend to (you should) use the available GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ace4a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa025dd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "In this notebook we will make use of [JAX](https://docs.jax.dev/en/latest/) and [Equinox](https://docs.kidger.site/equinox/) libraries, which adopt a more [functional](https://en.wikipedia.org/wiki/Functional_programming) approach to computing. *PyTorch* is only used for data mangling.\n",
    "\n",
    "# Dataset\n",
    "\n",
    "Images from the [CelebFaces Dataset](https://www.kaggle.com/datasets/arnrob/celeba-small-images-dataset) are downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749f06b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_dir = pathlib.Path(kagglehub.dataset_download(\"arnrob/celeba-small-images-dataset\"))\n",
    "print(\"Path to dataset files:\", imgs_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44016a7-b4a8-432f-ade7-bfad6e6f3ad1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "Some code to get the images ready for training. Essentially, we need to build a *PyTorch* `DataLoader` out of the images in the above directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead2d7a9-a0c6-4451-ad58-2d2b01ccebfa",
   "metadata": {
    "executionInfo": {
     "elapsed": 1239,
     "status": "ok",
     "timestamp": 1620743812042,
     "user": {
      "displayName": "AURORA COBO AGUILERA",
      "photoUrl": "",
      "userId": "02417368943911432830"
     },
     "user_tz": -120
    },
    "id": "mMf8KN88uFDm"
   },
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, img_dir, transform=None, n_samples=None):\n",
    "        \n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = []\n",
    "\n",
    "        # the names of *all* image files\n",
    "        print(f\"Scanning directory: {img_dir}\")\n",
    "        all_files = [os.path.join(img_dir, f) for f in os.listdir(img_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp'))]\n",
    "\n",
    "        # optionally limit the number of samples and shuffle for randomness\n",
    "        if n_samples is not None and n_samples < len(all_files):\n",
    "            self.image_files = random.sample(all_files, n_samples)\n",
    "        else:\n",
    "            self.image_files = all_files\n",
    "            random.shuffle(self.image_files) # Shuffle if using all files\n",
    "\n",
    "        # we don't have *actual* labels, but in the usual `Dataset` one is expected; it is set to 0 (dummy label) for all images\n",
    "        self.labels = [0] * len(self.image_files)\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        img_path = self.image_files[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "\n",
    "def get_celeba(\n",
    "        batch_size: int,\n",
    "        dataset_directory: str | pathlib.Path,\n",
    "        n: int | None = None,\n",
    "        data_subset: str = \"training\", # either 'training' or 'validation'\n",
    "    ) -> torch.utils.data.DataLoader:\n",
    "\n",
    "    # size of the images after resizing\n",
    "    img_size: tuple[int, int] = (64, 64)\n",
    "\n",
    "    train_transformation = transforms.Compose([\n",
    "        transforms.Resize(img_size), # *images* are resized,...\n",
    "        transforms.ToTensor(), # ...converted to *tensors*,...\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), # ...and normalized to have values in [-1, 1]\n",
    "    ])\n",
    "\n",
    "    # the path to the specific data subset (e.g., 'training')\n",
    "    actual_image_directory = pathlib.Path(dataset_directory) / data_subset\n",
    "    \n",
    "    if not actual_image_directory.is_dir():\n",
    "        raise ValueError(f\"Specified data_subset '{data_subset}' not found in '{dataset_directory}'.\")\n",
    "\n",
    "    train_dataset = CustomImageDataset(actual_image_directory, train_transformation, n_samples=n)\n",
    "\n",
    "    # a `DataLoader` is returned\n",
    "    return torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7f0043",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "Let us make a `DataLoader` looping through $10,000$ images in batches of size $32$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ea1951-fbb7-4be3-8ef8-1c8903cb46ad",
   "metadata": {
    "executionInfo": {
     "elapsed": 2033,
     "status": "ok",
     "timestamp": 1620743815085,
     "user": {
      "displayName": "AURORA COBO AGUILERA",
      "photoUrl": "",
      "userId": "02417368943911432830"
     },
     "user_tz": -120
    },
    "id": "t2RvRmKSuFDm"
   },
   "outputs": [],
   "source": [
    "trainloader = get_celeba(32, imgs_dir, n=10_000)\n",
    "trainloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ec0e1b-05af-41fa-9488-337f7e09ad92",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "A *PyTorch* `DataLoader` is ultimately an iterator...\n",
    "\n",
    "<font color='red'>TO-DO</font>: Get the first element from it. What is it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8815c082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b781217a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "<font color='red'>TO-DO</font>: Extract the first image from the above `DataLoader`. What is its type?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e0951d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "$d_x$ in the picture above would here be the overall number of pixels in an image, i.e., *width* $\\times$ *height* $\\times$ 3 channels (RGB)\n",
    "\n",
    "Let us write a convenience function to plot a *PyTorch* `Tensor` as an image. Notice that above's `transforms.Normalize` function is doing $\\frac{x - 0.5}{0.5} = 2(x-0.5)$...which must be undone before plotting in order to get an image ready for \"human consumption\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa878b7-ea84-42fe-99ca-aa7ab553db3a",
   "metadata": {
    "executionInfo": {
     "elapsed": 1091,
     "status": "ok",
     "timestamp": 1620743825129,
     "user": {
      "displayName": "AURORA COBO AGUILERA",
      "photoUrl": "",
      "userId": "02417368943911432830"
     },
     "user_tz": -120
    },
    "id": "a-iwMzzmuFDm"
   },
   "outputs": [],
   "source": [
    "def show_image(img):\n",
    "    \n",
    "    # image is \"unnormalized\"\n",
    "    img = img / 2 + 0.5\n",
    "    \n",
    "    # pytorch expects the channel dimension first whereas matplotlib expects it last\n",
    "    plt.imshow(np.transpose(img, (1, 2, 0)))\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9d553f",
   "metadata": {
    "tags": [
     "english"
    ]
   },
   "source": [
    "<font color='red'>TO-DO</font>: Plot the image you extracted above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f879523-bbce-4fbd-b587-86245f96e116",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 469
    },
    "executionInfo": {
     "elapsed": 17929,
     "status": "ok",
     "timestamp": 1620743844633,
     "user": {
      "displayName": "AURORA COBO AGUILERA",
      "photoUrl": "",
      "userId": "02417368943911432830"
     },
     "user_tz": -120
    },
    "id": "JwPRmrBMuFDo",
    "outputId": "f1fe85c8-2fc5-4fea-8d59-75d8432a7875"
   },
   "outputs": [],
   "source": [
    "# show_image(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae818b5a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "<font color='red'>TO-DO</font>: What happens if you skip the *unnormalization* part in `show_image`?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3243932",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "*PyTorch* provices a convenience function to stick together a bunch of images together. We can use it to plot a whole batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d114b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(torchvision.utils.make_grid(next(iter(trainloader))[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8f49ab",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "# Model\n",
    "\n",
    "The model encompasses two components, the *encoder/compressor* (implementing function `f_enc` in the above picture) and the *decoder/decompressor* (implementing function `f_dec`). In the middle of them we have the \"compressed space\", known as the *latent* space. We can choose its dimension (size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ca7e98-954f-4239-afbc-bdc9f9d06a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_z = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78790467",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "## Encoder\n",
    "\n",
    "A `class` defining the architecture of the *encoder* (i.e., the compressor). This is assuming $64 \\times 64$ images. If that's not the case, tweaks are required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae318b0f-dc69-4161-af54-1b91a5308cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(eqx.Module):\n",
    "\n",
    "    layers: list\n",
    "\n",
    "    def __init__(self, d_z: int, input_channels: int = 3, rng_key: jr.PRNGKey = jr.PRNGKey(42)):\n",
    "\n",
    "        key1, key2, key3, key4, key5, key6 = jr.split(rng_key, 6)\n",
    "\n",
    "        self.layers = [\n",
    "            eqx.nn.Conv2d(in_channels=input_channels, out_channels=32, kernel_size=4, stride=2, padding=1, key=key1),\n",
    "            jax.nn.relu,\n",
    "            eqx.nn.Conv2d(in_channels=32, out_channels=32, kernel_size=4, stride=2, padding=1, key=key2),\n",
    "            jax.nn.relu,\n",
    "            eqx.nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2, padding=1, key=key3),\n",
    "            jax.nn.relu,\n",
    "            eqx.nn.Conv2d(in_channels=64, out_channels=64, kernel_size=4, stride=2, padding=1, key=key4),\n",
    "            jax.nn.relu,\n",
    "            eqx.nn.Conv2d(in_channels=64, out_channels=256, kernel_size=4, stride=1, padding=0, key=key5),\n",
    "            jnp.ravel,\n",
    "            eqx.nn.Linear(256, 2*d_z, key=key6),\n",
    "            lambda x: x.at[d_z:].set(jax.nn.softplus(x[d_z:]))\n",
    "        ]\n",
    "    \n",
    "    def __call__(self, x):\n",
    "\n",
    "        for layer in self.layers:\n",
    "\n",
    "            x = layer(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3a8ece",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "Let us instantiate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55097455-1892-43cf-b0e8-9acbd6156a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(d_z=d_z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b07ed8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "Object `encoder` ultimately behaves as a function accepting an image (in the form of an array) as input, and returning a vector of size $2 \\times d_z$ yielding the corresponding mean and standard deviation (stacked together) of a Gaussian distribution in the latent space. Indeed, the encoder not only gives you some $z$ in the compressed space, but also a measure of its uncertainty.\n",
    "\n",
    "Let us call it on the first image from the first batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de4abe8-64ad-4805-9f83-7cb9ad709428",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_mean_std = encoder(next(iter(trainloader))[0][0].numpy())\n",
    "z_mean, z_std = jnp.split(z_mean_std, 2)\n",
    "z_mean, z_std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4fa894",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "Notice that, as required, the standar deviations are non-negative."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c44628d8-8fef-4686-bb6f-740a0c0e5b19",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "## Decoder\n",
    "\n",
    "The architecture for the *decoder*, i.e., the decompressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b626a373-bdc2-4ee3-b09c-708e44f137b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(eqx.Module):\n",
    "\n",
    "    layers: list\n",
    "\n",
    "    def __init__(self, d_z: int, input_channels: int = 3, rng_key: jr.PRNGKey = jr.PRNGKey(42)):\n",
    "\n",
    "        key1, key2, key3, key4, key5, key6 = jr.split(rng_key, 6)\n",
    "\n",
    "        self.layers = [\n",
    "            eqx.nn.Linear(d_z, 256, key=key1),\n",
    "            lambda x: jnp.reshape(x, (256, 1, 1)),\n",
    "            jax.nn.relu,\n",
    "            eqx.nn.ConvTranspose2d(in_channels=256, out_channels=64, kernel_size=4, stride=1, padding=0, key=key2),\n",
    "            jax.nn.relu,\n",
    "            eqx.nn.ConvTranspose2d(in_channels=64, out_channels=64, kernel_size=4, stride=2, padding=1, key=key3),\n",
    "            jax.nn.relu,\n",
    "            eqx.nn.ConvTranspose2d(in_channels=64, out_channels=32, kernel_size=4, stride=2, padding=1, key=key4),\n",
    "            jax.nn.relu,\n",
    "            eqx.nn.ConvTranspose2d(in_channels=32, out_channels=32, kernel_size=4, stride=2, padding=1, key=key5),\n",
    "            jax.nn.relu,\n",
    "            eqx.nn.ConvTranspose2d(in_channels=32, out_channels=input_channels, kernel_size=4, stride=2, padding=1, key=key6),\n",
    "            jax.nn.tanh\n",
    "        ]\n",
    "    \n",
    "    def __call__(self, x):\n",
    "\n",
    "        for layer in self.layers:\n",
    "\n",
    "            x = layer(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b974ad8d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "`decoder` behaves as a function acting on vectors in the latent space (of dimension $d_z$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a489dd3-9dea-4c0e-96d0-ee1218dd995d",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(d_z=d_z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95f9860",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "Let us draw a sample from the above mean and standard deviation (in this programming *functional* paradigm, we must pass a pseudo-random numbers generator key/seed, here `jr.PRNGKey(42)` every time a random number is to be produced)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef65385",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = dist.Normal(loc=z_mean, scale=z_std).sample(jr.PRNGKey(42))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b03c4f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "...and *decode* it to (maybe?) get back the original image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ccc9b4-db7d-4db4-afb8-f591549a766b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_est = decoder(z)\n",
    "x_est.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfb9b92",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "<font color='red'>TO-DO</font>: Visualize the image. What is wrong?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2cb2fc00-a210-420c-9f80-f54e13597cd8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "# Training\n",
    "\n",
    "Some hyperparameters that can be tweaked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f49146-5486-4378-917d-000a1559958f",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "# n_epochs = 40\n",
    "n_epochs = 10\n",
    "d_z = 75"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f658ade-0013-4fd7-9f3d-6e05b1781330",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "Neural networks for the decoder and encoder are instantiated *and* initialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0a0bd6-a933-4f1f-9e8e-671a9dc12302",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(d_z=d_z)\n",
    "decoder = Decoder(d_z=d_z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f09554",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "For the sake of convenience, we will gather together both things in a Python `tuple`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6bd0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = encoder, decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e3604c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "We define the loss function (the one to be minimized). Prior to that, and for the sake of clarity, we also define [Kullback-Leibler divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence), which provides a way to tell how different two distributions are. Just **skip** this code for now (this course) since this has to do with technical details about a VAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd732561-c82d-4d25-813c-f5ff27da4e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_loss(mean: Float[Array, 'feature'], sd: Float[Array, 'feature']) -> Float[Array, '']:\n",
    "\n",
    "    return -0.5 * jnp.sum(1 + 2*jnp.log(sd) - mean**2 - sd**2)\n",
    "\n",
    "def loss(model, x: Float[Array, 'batch channel width height'], rng_key) -> Float[Array, '']:\n",
    "\n",
    "    # never mind for now...but this is variance assumed for the decoded `x`\n",
    "    x_var = 0.1\n",
    "\n",
    "    encoder, decoder = model\n",
    "    z_mean_std = jax.vmap(encoder)(x)\n",
    "\n",
    "    z = dist.Normal(loc=z_mean_std[:, :d_z], scale=z_mean_std[:, d_z:]).sample(rng_key)\n",
    "    \n",
    "    x_pred = jax.vmap(decoder)(z)\n",
    "\n",
    "    log_likelihood = dist.Normal(loc=x_pred, scale=jnp.sqrt(x_var)).log_prob(x).sum()\n",
    "\n",
    "    kl_divergence = jax.vmap(kl_loss)(z_mean_std[:, :d_z], z_mean_std[:, d_z:]).sum()\n",
    "    \n",
    "    return -log_likelihood + kl_divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb4b337",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "The loss function is just, well...a function, that you can actually call like any other function. Let us get a batch of images from the `DataLoader` above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53720ba5-8ffb-4247-bbe6-90505b0cc807",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, _ = next(iter(trainloader))\n",
    "images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cf9033",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "<font color='red'>TO-DO</font>: Explain the size of the above `Tensor`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b41aa1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "<font color='red'>TO-DO</font>: Call the loss function (with the above `model`) on the images. Looking at the above definition, `loss` expects:\n",
    "\n",
    "- the model,\n",
    "\n",
    "- either a *numpy* or a *jax* array, so you must convert `images`, which is a *PyTorch* tensor (you can use the method `numpy()` on `images`)\n",
    "\n",
    "- a pseudo-random numbers generator key (you can use again `jr.PRNGKey(42)`...or something else) to produce the required random numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac0d967",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "A nice thing about *JAX*/*Equinox* is that if you have a (Python) function, you can easily get the [gradient](https://en.wikipedia.org/wiki/Gradient) of that function by using `jax.grad`. In this case, since we are using *Equinox*, we call the equivalent *wrapper* `eqx.filter_grad`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c23844d",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_loss = eqx.filter_grad(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db837599",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "Now, `grad_loss` is a function taking the same arguments as `loss`, so you can...\n",
    "\n",
    "<font color='red'>TO-DO</font>: Call the function just like you call `loss` above. What do you get? Keep in mind we are computing the *gradient* of the loss function!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fd44da",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "Now we make a function that packs the actions that must be carried out, during training, on every batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7891c06-2f94-4a14-a18b-ed23561b24eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@eqx.filter_jit\n",
    "def take_step(model, opt_state, x: jax.Array, rng_key):\n",
    "\n",
    "    loss_value, grads = eqx.filter_value_and_grad(loss)(model, x, rng_key)\n",
    "\n",
    "    updates, opt_state = optim.update(grads, opt_state, model)\n",
    "    model = eqx.apply_updates(model, updates)\n",
    "    \n",
    "    return model, opt_state, loss_value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0f6987ff-52f3-4dcf-bc66-36f9ae51508f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "## Loop\n",
    "\n",
    "Let's go for the training loop. You'll notice that the evolution of the loss is somehow *bumpy*. That's OK. Also, the first iteration might take a while (since the data are being read into memory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a330b7da-e59e-4fc0-97eb-707e501b4d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = optax.adam(learning_rate)\n",
    "opt_state = optim.init(eqx.filter(model, eqx.is_array))\n",
    "\n",
    "key = jr.PRNGKey(42)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    key, subkey = jr.split(key)\n",
    "\n",
    "    for x, _ in trainloader:\n",
    "\n",
    "        model, opt_state, loss_value = take_step(model, opt_state, x.numpy(), subkey)\n",
    "\n",
    "    print(epoch, loss_value)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2f470ea2-3d8e-43ee-9672-0020cb957629",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "# Results\n",
    "\n",
    "Let us look a the first picture in the last batch processed in the training loop (still in `x`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f869323-3c22-42e3-89e9-5a85f4ca0752",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5228012c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "Let us encode it into the latent space and decode it back. Formally, the decoder returns the mean and standard deviation (stacked together) in the data space but, for the sake of simplicity, we can take the former as if it were a sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e19e3b4-ea28-4cca-a5d3-493cf6e00e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder, decoder = model\n",
    "show_image(decoder(encoder(x[0].numpy())[:d_z]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eaabf462-8cfe-40a3-af96-6eb12e302225",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "# Experiments\n",
    "\n",
    "<font color='red'>TO-DO</font>: Generate a couple of new images by drawing samples in the *latent space* and calling the decoder on them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5c5474",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "<font color='red'>TO-DO</font>: Train for a few more epochs to try and improve the quality of the *reconstructions*. Does it get better?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfa8088",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "<font color='red'>TO-DO</font>: Train with very few images, say 10. The number of images you use for training (out of the overall number in the dataset) is controlled by the `n` parameter to the function `get_celeba` above. Use a much larger number of epochs, say 500, or the model won't have seen enough examples to learn anything. Then, generate a few images and compare them. What do you observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62185e54",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "<font color='red'>TO-DO</font>: Play around with the dimension of the latent space. Can you get good results with a small dimension, say $d_z=10$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d0b3b2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "# Sample questions\n",
    "\n",
    "## What is a typical effect of using a *smaller* latent dimension (for example, going down to 10)?\n",
    "- [ ] The model always trains faster and becomes perfect\n",
    "- [ ] The model has less capacity to capture details, so reconstructions may become blurrier or lose information\n",
    "- [ ] The model stops using the decoder network\n",
    "- [ ] The model cannot be trained at all\n",
    "\n",
    "## Why do we set random seeds (for example, for numpy or PyTorch) at the beginning?\n",
    "- [ ] To make the training run only once\n",
    "- [ ] To avoid using the GPU by mistake\n",
    "- [ ] To make the results more reproducible when we run the code again\n",
    "- [ ] To prevent the code from using any random numbers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
