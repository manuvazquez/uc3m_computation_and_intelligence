{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75fbfc22",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "> Modelos Extensos de Lenguaje y Generación Aumentada por Recuperación\n",
    "\n",
    "Los [modelos extensos de lenguaje](https://es.wikipedia.org/wiki/Modelo_extenso_de_lenguaje) (LLMs, del inglés *Large Language Models*), que dan vida a herramientas como *ChatGPT*, son buenos para *generar* texto, pero no siempre disponen de la información más reciente o a datos específicos que nos interesan. Aquí es donde entra en juego la [generación aumentada por recuperación](https://es.wikipedia.org/wiki/Generaci%C3%B3n_aumentada_por_recuperaci%C3%B3n) (RAG, del inglés, *Retrieval-augmented generation*): en lugar de depender solo de lo que el modelo \"recuerda\", podemos conectarlo a una fuente externa de conocimiento (como documentos, notas o una base de datos). Con RAG, el modelo primero recupera los fragmentos de información más relevantes, y luego los usa para generar una respuesta precisa y útil. Este enfoque hace que los LLMs sean más fiables, estén más actualizados y sean más útiles para aplicaciones del mundo real como chatbots, asistentes y sistemas de búsqueda.\n",
    "\n",
    "Para constuir de forma sencilla un RAG, usaremos [LangChain](https://github.com/langchain-ai/langchain), que es un framework de código abierto para implementar rápidamente aplicaciones y agentes basados en LLMs usando componentes e integraciones ya preparados.\n",
    "\n",
    "# Preparación del entorno\n",
    "\n",
    "En principio, podrías ejecutar el *notebook* en Colab o localmente. ¿El *notebook* se está ejecutando en *Colab*?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e89d3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    running_in_colab = True\n",
    "except ImportError:\n",
    "    running_in_colab = False\n",
    "\n",
    "running_in_colab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c32f8e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "Si estás en *Colab*, es conveniente cambiar al entorno GPU...o ponerte cómodo mientras la CPU hace los cálculos.\n",
    "\n",
    "Si no estás ejecutando en *Colab*, puede que quieras elegir una GPU si hay varias disponibles. Ignora esto si estás ejecutando en *Colab*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933e61b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not running_in_colab:\n",
    "\n",
    "    import os\n",
    "    os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42a9f7e",
   "metadata": {
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "## Ollama\n",
    "\n",
    "Vamos a usar un LLM disponible gratuitamente (entrenado por otra persona). Para *ejecutarlo*, usaremos [Ollama](https://ollama.com/), que es un software para\n",
    "\n",
    "> poner en marcha y ejecutar modelos de lenguaje grandes\n",
    "\n",
    "Funciona con un modelo cliente-servidor, así que debemos arrancar el servidor antes de enviar cualquier petición.\n",
    "\n",
    "- Si estás en *Colab*, se instalarán los paquetes necesarios. Puede que veas algunos <font color='red'>ERROR/WARNING</font>s. Ignóralos. y debería funcionar sin problemas.\n",
    "\n",
    "- Si **no** estás en *Colab* deberías instalar *ollama* por tu cuenta (junto con todos los paquetes de *python* necesarios)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52d3dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import pathlib\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "if running_in_colab:\n",
    "    !pip uninstall -y langchain 2>/dev/null || true # to avoid conflicts with new langchain version\n",
    "    !pip -q install -U langchain-core langchain-community langchain-text-splitters langchain-chroma langchain-huggingface langchain-ollama llama-index-core chromadb unstructured\n",
    "    !curl -fsSL https://ollama.com/install.sh | sh\n",
    "    !nohup ollama serve > /dev/null 2>&1 &\n",
    "\n",
    "else:\n",
    "\n",
    "    log_file = pathlib.Path('ollama.log').open('w')\n",
    "    proc = subprocess.Popen(\n",
    "        [str(shutil.which('ollama') or pathlib.Path.home() / 'ollama' / 'bin' / 'ollama'), 'serve'],\n",
    "        stdout=log_file,\n",
    "        stderr=subprocess.STDOUT,\n",
    "        start_new_session=True,\n",
    "    )\n",
    "    \n",
    "# give it some time to start\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a20109",
   "metadata": {
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "El servidor LLM (*ollama*) debería estar ya en marcha y funcionando."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ba9a8a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "## Bibliotecas de Python\n",
    "\n",
    "El resto de `import`s necesarios se \"centralizan\" aquí."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e565ab39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# embedding\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "import ollama\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "\n",
    "# database\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "# model\n",
    "from langchain_ollama import OllamaLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3e3e32",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "# Datos\n",
    "\n",
    "Vamos a descargar algunos datos. Aquí usamos el relato corto [Bartleby, the Scrivener](https://en.wikipedia.org/wiki/Bartleby,_the_Scrivener) de Herman Melville (disponible libremente en [Project Gutenberg](https://www.gutenberg.org/)), pero puedes usar cualquier documento de *texto plano* que prefieras (simplemente asegúrate de colocarlo en el directorio correcto o de modificar el código según haga falta)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede6928a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "bartleby_url = 'https://www.gutenberg.org/ebooks/11231.txt.utf-8'\n",
    "response = requests.get(bartleby_url)\n",
    "response.raise_for_status()\n",
    "\n",
    "data_dir = pathlib.Path('data')\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "book_file = data_dir / 'bartleby.txt'\n",
    "\n",
    "with book_file.open('w', encoding='utf-8') as f:\n",
    "    f.write(response.text)\n",
    "\n",
    "print(f'Downloaded book to {book_file}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866bcf30",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "Vamos a cargar cada documento en el directorio `data` dado como un `Document` de *LangChain* (devuelve una `list` con todos los documentos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1fb7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = DirectoryLoader('data').load()\n",
    "type(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07232473",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "<font color='red'>TO-DO</font>: ¿Cuántos documentos tienes? Revisa el contenido de uno de ellos. ¿Cuál es su tipo en Python? Intenta acceder al texto interno (el atributo importante es `page_content`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886d3def",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "Cada `Document` de arriba tiene que convertirse en un vector de números reales de **tamaño fijo**. Antes conseguíamos esto con *Bag-of-words*, pero podemos utilizar distintos modelos (ya entrenados) para esta tarea y aquí usaremos un *sentence transformer* (los detalles se verán en otros cursos). Este modelo es mucho más sofisticado (preserva información semántica y de contexto) que *Bag-of-words*. Los vectores resultantes de números reales de **tamaño fijo** suelen llamarse [embeddings](https://en.wikipedia.org/wiki/Embedding_(machine_learning))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0864ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "embed_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469f36a0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "<font color='red'>TO-DO</font>: Haz el embedding de cualquier frase que quieras (simplemente llama al `embed_model` de arriba con cualquier texto) y el de otra muy parecida. Échale un vistazo a sus vectores correspondientes y compáralos de alguna manera."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477ae866",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "<font color='red'>TO-DO</font>: Prueba (unos cuantos) [modelos diferentes](https://huggingface.co/sentence-transformers/models) para hacer el embedding de los `Document`s. ¿Ves alguna diferencia (velocidad, rendimiento...)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fd5993",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "\n",
    "El número de *tokens* que se pueden pasar como entrada a un *sentence transformer* es limitado (eso no ocurría con *Bag-of-words*). La documentación del modelo [all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) usado arriba dice\n",
    "\n",
    "> un texto de entrada de más de 256 trozos se trunca.\n",
    "\n",
    "Para lidiar con esto, los documentos se dividen en secuencias de (aquí) 500 *caracteres* que se **solapan** (un *token* contiene varios). Esta división de los documentos también es buena para tener una *granularidad* más fina cuando buscamos información en el documento, ya que el RAG puede localizar y devolver el segmento específico donde aparece la información relevante en lugar de un documento largo entero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9789e425",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "documents = splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51381918",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "<font color='red'>TO-DO</font>: Después de la división, ¿cuántos documentos tienes? ¿Qué longitudes tienen los tres primeros?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e75ad50",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "# Almacenar los documentos\n",
    "\n",
    "Vamos a utilizar una base de datos [Chroma](https://www.trychroma.com/) para almacenar los *documentos embebidos*. Al instanciar el objeto correspondiente, necesitamos pasarle el modelo de embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5fa118",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma.from_documents(\n",
    "    documents, embedding=embed_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b87fe9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "Fíjate en que el método [`from_documents`](https://reference.langchain.com/python/integrations/langchain_chroma/#langchain_chroma.Chroma.from_documents) de arriba hace el embedding y almacena los documentos de una sola vez.\n",
    "\n",
    "<font color='red'>TO-DO</font>: ¿Qué tipo de base de datos es [Chroma](https://www.trychroma.com/)? ¿En qué se diferencia de las bases de datos tradicionales?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64a1566",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "Puedes explotar la base de datos [Chroma](https://www.trychroma.com/) para buscar frases *similares* a una dada. Cada búsqueda en la base de datos puede (en principio, lo hará) devolver varios resultados (tantos como se soliciten a través del parámetro `k`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d01befc",
   "metadata": {},
   "outputs": [],
   "source": [
    "hits = vectordb.similarity_search(\n",
    "    \"I would prefer not to do it\", k=3\n",
    ")\n",
    "len(hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41635af",
   "metadata": {},
   "outputs": [],
   "source": [
    "for h in hits:\n",
    "    print(h.page_content[:120], '\\n---')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be9af84",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "<font color='red'>TO-DO</font>: Busca:\n",
    "\n",
    "- Una frase que esté cerca de algo que sabes que está ahí (en el texto).\n",
    "\n",
    "- Una frase que (probablemente) no esté."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13eafe2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "# LLM\n",
    "\n",
    "## Modelo\n",
    "\n",
    "Vamos a elegir un modelo. Puedes ver [aquí](https://ollama.com/library?sort=popular) cuáles están disponibles libremente. Empecemos con"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c0bf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = 'llama3:8b'\n",
    "# model_name = 'deepseek-r1:8b'\n",
    "model_name = 'llama3.2:1b' # faster download"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69686b05",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "AVISO: algunos modelos también mostrarán el proceso de pensamiento (mediante etiquetas `<think>`), lo que puede no ser muy conveniente para ciertas tareas y es posible que queramos filtrarlo.\n",
    "\n",
    "Solo hay que descargar el modelo una vez (...por sesión si utilizas *Colab*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581bd726",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama.pull(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b25f9a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "Una vez que el servidor está en marcha, se puede acceder a un modelo (previamente descargado) a través de un objeto de `langchain`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb35c4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OllamaLLM(model=model_name, temperature=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7361c55",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "<font color='red'>TO-DO</font>: Pregúntale al modelo LLM lo que quieras (usando el método `invoke`), e.g., \"¿Qué es \"backpropagation\"? o \"¿Qué es Rick y Morty?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623555cf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "<font color='red'>TO-DO</font>: Pregúntale al modelo LLM sobre el personaje principal de la novela, Bartleby. Esto lo hacemos **antes** de *aumentar* lo que el LLM sabe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a809855",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "## Plantillas\n",
    "\n",
    "Si piensas interactuar con un LLM usando siempre un determinado estilo de *prompting*, puedes crear una plantilla para ello."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9e0e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt = ChatPromptTemplate([\n",
    "    ('system', 'Try to rhyme every answer you give.'),\n",
    "    ('human', '{user_input}'),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326fd711",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "Luego se construye una cadena o *chain* (se utiliza como arriba) que *envía* (`|` puede interpretarse como $\\to$) la plantilla al LLM y pasa el resultado a un *parser* que devuelve un `str`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0f274e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = chat_prompt | llm | StrOutputParser()\n",
    "print(chain.invoke('Explain backpropagation in one paragraph.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78355fe5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "<font color='red'>TO-DO</font>: Experimenta con las instrucciones `system` de arriba."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ef6b4e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "<font color='red'>TO-DO</font>: Crea una nueva *chain* (guardada en una variable nueva) que se quede solo con la última palabra de cada frase y descarte el resto. Para simplificar (no necesitamos instrucciones `system`), puedes usar `PromptTemplate.from_template`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7200b4cb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "<font color='red'>TO-DO</font>: Crea otra *chain* más encadenando (`|`) las dos anteriores para obtener una *chain* que devuelva las palabras que riman en la salida de arriba."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ffe30c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "# \"Recuperador\" (*retriever*) + LLM\n",
    "\n",
    "`langchain` proporciona una forma de construir un RAG completo de principio a fin. Solo necesitamos:\n",
    "\n",
    "- un LLM,\n",
    "\n",
    "- una base de datos de documentos (que se organiza usando `as_retriever` para que `langchain` sepa cómo \"hablar\" con ella), y\n",
    "\n",
    "- un *prompt* que tenga en cuenta esta última.\n",
    "\n",
    "Empecemos con el *prompt*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb746993",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = (\n",
    "    'Use the given context to answer the question. '\n",
    "    'If you don\\'t know, say so. '\n",
    "    'Keep the answer under three sentences.\\n\\n'\n",
    "    'Context:\\n{context}'\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [('system', system_prompt), ('human', '{input}')]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36196d80",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "Una función que simplemente une los documentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6e4304",
   "metadata": {},
   "outputs": [],
   "source": [
    "join_docs = RunnableLambda(\n",
    "    lambda docs: '\\n\\n'.join(d.page_content for d in docs)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f9cbdb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "`context` e `input` son las entradas para (*rellenar*) el `prompt`, que se envía al `llm`, cuya salida se *parsea* para obtener la salida final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdaeeaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "piped_rag_chain = (\n",
    "    {                                             #  a `dict` for the prompt\n",
    "        'context': vectordb.as_retriever(search_kwargs={\"k\": 4}) | join_docs,\n",
    "        'input':   RunnablePassthrough(),\n",
    "    }\n",
    "    | prompt                                      # fill {context} + {input}\n",
    "    | llm                                         # generate answer\n",
    "    | StrOutputParser()                           # ChatMessage → str\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e54c70",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "<font color='red'>TO-DO</font>: Vuelve a preguntarle a esta *chain* (usando el método `invoke`) las mismas preguntas que hiciste arriba (Bartleby y lo que preguntaste antes). Compara las respuestas (ahora, **después** de *aumentar* el LLM). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a6553d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not running_in_colab:\n",
    "    proc.terminate()   # or proc.kill() for a hard stop\n",
    "    proc.wait()\n",
    "# log.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707435cc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "<font color='red'>TO-DO</font> : Prueba un modelo de lenguaje distinto (hay un par de ellos comentados en el código donde se define `model_name`). Ojo con el tamaño del modelo (*7b*, *8b*...). Los modelos grandes pueden tardar **mucho** en dar una respuesta... o incluso colapsar la memoria de la GPU. El sufijo *b* en esos modelos significa *billones* (americanos). El *Toy GPT* que entrenamos antes tenía... 0.2 **m**illones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeba33fd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "# Preguntas de ejemplo\n",
    "\n",
    "## ¿Cuál es el beneficio principal de añadir recuperación (retrieval) a un sistema basado en LLM?\n",
    "- [ ] Elimina la necesidad de diseñar *prompts*.\n",
    "- [ ] Aporta primero contexto externo relevante, haciendo las respuestas más precisas y actualizadas.\n",
    "- [ ] Reescribe permanentemente los pesos del modelo con hechos nuevos.\n",
    "- [ ] Obliga al modelo a responder solo con citas.\n",
    "\n",
    "## ¿Qué representa un *embedding* para un fragmento de texto?\n",
    "- [ ] Una compresión perfecta y sin pérdida del texto original.\n",
    "- [ ] El conteo únicamente de palabras y signos de puntuación.\n",
    "- [ ] Un vector numérico que captura el significado de modo que textos similares queden cerca entre sí.\n",
    "- [ ] Una lista de palabras clave en orden alfabético.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cei",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
