{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77cf01b8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "> Image classification\n",
    "\n",
    "Image classification is a very important application of machine learning: it’s what allows computers to “see.” From recognizing handwritten digits to identifying animals in photos, the goal is to teach a model to assign labels to images based on what it learns from examples. In this notebook, you’ll build and test a simple image classifier using a small dataset, exploring how machines can learn to tell apart two different kinds of objects (e.g., muffins and dogs!!).\n",
    "\n",
    "# Setup\n",
    "\n",
    "All the required `import`s come here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb36087",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import zipfile\n",
    "\n",
    "import fastai\n",
    "from fastai.vision.all import *\n",
    "from fastai.torch_core import set_seed\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb990b7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "In principle, you could run the notebook either in *Colab* or locally. Is the notebook running in *Colab*?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633e9bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    running_in_colab = True\n",
    "except ImportError:\n",
    "    running_in_colab = False\n",
    "\n",
    "running_in_colab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a080ac",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "If not running in *Colab*, you might want to choose a GPU if several are available. Ignore, if you are running in *Colab*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be2dd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not running_in_colab:\n",
    "\n",
    "    import os\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3f6bb5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "GPU acceleration available?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d231bd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074024c9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "# Data\n",
    "\n",
    "We'll pull the [Muffin vs chihuahua](https://www.kaggle.com/datasets/samuelcortinhas/muffin-vs-chihuahua-image-classification/data) dataset from [Kaggle](https://www.kaggle.com/https://www.kaggle.com/). Let us download the corresponding *.zip* file and uncompress it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7480ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data will live inside this inside directory\n",
    "data_dir = pathlib.Path('muffin_chihuahua')\n",
    "\n",
    "# it it doesn't exist (from a previous run)...\n",
    "if not data_dir.exists():\n",
    "\n",
    "    # ...it is created\n",
    "    data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    # data is downloaded as a zip file, that will be named\n",
    "    zip_file = data_dir / 'muffin-vs-chihuahua-image-classification.zip'\n",
    "\n",
    "    # actual download\n",
    "    !curl -L -o {zip_file} https://www.kaggle.com/api/v1/datasets/download/samuelcortinhas/muffin-vs-chihuahua-image-classification\n",
    "\n",
    "    # data is unzipped inside the data directory\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zf:\n",
    "        zf.extractall(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8359195",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "What's in the directory?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f681e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(data_dir.iterdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd684ac3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "Hence, besides the *.zip* we have just downloaded, we have the usual *train* and *test* sets, and inside each one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7540c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in ['train', 'test']:\n",
    "    print(f\"{e}:\")\n",
    "    for c in ['chihuahua', 'muffin']:\n",
    "        n = len(list((data_dir / e / c).iterdir()))\n",
    "        print(f\"  {c}: {n}\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34243743",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "We'll focus on the training data only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0029d184",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = data_dir / 'train'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c4c2b5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "## Data loading\n",
    "\n",
    "We'll make use of two well-known deep learning libraries: [fastai](https://github.com/fastai/fastai) and [PyTorch](https://pytorch.org/).\n",
    "\n",
    "We'll let *fastai* read images from the class folders and create a train/validation split automatically. What the code below does is essentially:\n",
    "\n",
    "- Define the transformation(s) to be applied to every individual *item* (i.e., image). We'll resize every image to a manageable size (e.g., 256×256).\n",
    "\n",
    "- Define the transformation(s) to be applied to every *batch*. These include, crucially, *augmentations* (check the [docs](https://docs.fast.ai/vision.augment.html#aug_transforms) for what's possible).\n",
    "\n",
    "- Instantiate a `DataBlock` object, which is a \"blueprint\" specifying how to access the dataset.\n",
    "\n",
    "- Use the `DataBlock` to obtain the actual `DataLoader`s (*PyTorch* objects intended to loop through the whole dataset one batch at a time), one for the *training* set, and one for the *validation* set.\n",
    "\n",
    "Notice two *resize* operations are applied: one in `item_tfms` and another in `batch_tfms`. Never mind the details, but the latter is the one actually determining the size of the images the model will be trained on, and the former is just a *presizing* step that helps in avoiding artifacts in the transformed images (no need, but full story can be found in the [fastai book](https://nbviewer.org/github/fastai/fastbook/blob/master/05_pet_breeds.ipynb#Presizing))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf807c9",
   "metadata": {
    "id": "dls"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "# a transformation to apply to every item\n",
    "item_tfms = [Resize(256)]\n",
    "\n",
    "# a `list` of transformations to apply to every *batch*\n",
    "batch_tfms = [*aug_transforms(size=224, max_warp=0), Normalize.from_stats(*imagenet_stats)]\n",
    "\n",
    "dblock = DataBlock(\n",
    "    blocks=(ImageBlock, CategoryBlock),\n",
    "    get_items=get_image_files,\n",
    "    get_y=parent_label,\n",
    "    splitter=RandomSplitter(valid_pct=0.2, seed=42),  # TODO: try a different seed\n",
    "    item_tfms=item_tfms,\n",
    "    batch_tfms=batch_tfms\n",
    ")\n",
    "\n",
    "dls = dblock.dataloaders(data_root, bs=batch_size)\n",
    "dls.show_batch(max_n=12, figsize=(8,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0fdf9c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "Among the *batch* transformations (those passed through `batch_tfms`) we have the function `aug_transforms`. This performs [data augmentation](https://en.wikipedia.org/wiki/Data_augmentation). The idea is: a picture of a dog is still a picture of a dog if you rotate it a little bit...or warp it a little bit...or crop it a little bit (e.g., you keep only the head). Then, each of these *versions* of the same picture can be used in training the model, and we are *artificially* generating (making up) training data (actually, an infinite amount!!...since the amount of rotation/warping/whatever will be selected at random at every iteration of the training procedure). *Data augmentation* is a very common trick in computer vision because it's very useful (and computationally cheap).\n",
    "\n",
    "<font color='red'>TO-DO</font>: Notice there is an extra (besides that implied by the use of *data augmentation*) source of *randomness* here: the `seed` passed to `RandomSplitter`. What's that for?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ba926d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "# Model training\n",
    "\n",
    "We will leverage [transfer learning](https://en.wikipedia.org/wiki/Transfer_learning) and exploit a pre-trained model such as *mobilenet_v3_small*. More on this in future courses...but *transfer learning* allows us to reuse a model trained for some task onto a different (related) task (in this case, we are leveraging a model trained on classifying images in [imagenet](https://www.image-net.org/)...which has a 1,000 categories).\n",
    "\n",
    "The *metric* we are interested in is *accuracy*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b9b4fd",
   "metadata": {
    "id": "train"
   },
   "outputs": [],
   "source": [
    "learn = vision_learner(dls, mobilenet_v3_small, metrics=accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b9fa50",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "Let us train the model for a few *epochs*. Every *epoch* loops through the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cc53af",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fine_tune(\n",
    "    epochs=3,\n",
    "    base_lr=3e-3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4708cb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "<font color='red'>TO-DO</font>: What percentage of the time does the model get it wrong?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162dac2d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "<font color='red'>TO-DO</font>: What happens if you train again (by, essentially, running all over the last couple of cells)? Do you get the same results? Why not? (*Hint*: how are the parameters of the model initialized every time you call `vision_learner`?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25fa2de",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "# Results\n",
    "\n",
    "Let us show some predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cc599c",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.show_results(max_n=9, figsize=(8,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a274e98",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "Even though the performance is really good, it's interesting to see where the model struggles.\n",
    "Let's look at the [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix) to see whether the model makes more mistakes when dealing with a certain class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5abcf44",
   "metadata": {
    "id": "eval"
   },
   "outputs": [],
   "source": [
    "interp = ClassificationInterpretation.from_learner(learn)\n",
    "interp.plot_confusion_matrix(figsize=(4,4), dpi=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bae06b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "<font color='red'>TO-DO</font>: At the sight of this confusion matrix, does the performance of the model depends on whether the input is from one class or the other?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adc2848",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "Let us look at some of the images the model had the most troubles with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a5b1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "interp.plot_top_losses(4, nrows=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bf3c5f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "<font color='red'>TO-DO</font>: Could you guess the class on your own?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de8e186",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "# The model on you own images\n",
    "\n",
    "Pick a photo (muffin, chihuahua...or something else) and see what the model predicts. **The GUI requires *Colab***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37aacc2",
   "metadata": {
    "id": "predict"
   },
   "outputs": [],
   "source": [
    "if running_in_colab:\n",
    "\n",
    "    try:\n",
    "        from google.colab import files\n",
    "        up = files.upload()\n",
    "        for fn in up:\n",
    "            img = PILImage.create(fn)\n",
    "            pred,pred_idx,probs = learn.predict(img)\n",
    "            print(f'{fn} -> {pred}; probs={probs.tolist()}')\n",
    "            display(img.to_thumb(256,256))\n",
    "    except Exception as e:\n",
    "        print('Local environment or no file uploaded:', e)\n",
    "\n",
    "else:\n",
    "\n",
    "    print(\"The GUI requires Colab.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac23d4f2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "The probability you get is that of the first label, i.e., that of a *chihuahua*.\n",
    "\n",
    "<font color='red'>TO-DO</font>: What happens if you give the model something that is nor a muffin nor a chihuahua?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7374cb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "# Hyperparameter tuning\n",
    "\n",
    "## Augmentation\n",
    "\n",
    "Let us try and play around with *data augmentation*...\n",
    "\n",
    "<font color='red'>TO-DO</font>: Explore other types of augmentation by passing different parameters to function `aug_transforms`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbff19c1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# aug = aug_transforms(..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80648a8d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "We need to make another (different) `DataBlock`, which can be built by modifying the old one. Updated `DataLoaders` are obtained from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04cdf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_dblock = dblock.new(item_tfms=item_tfms,batch_tfms=[*aug, Normalize.from_stats(*imagenet_stats)])\n",
    "aug_dls = aug_dblock.dataloaders(data_root, bs=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93bcf7b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "Let us visualize a batch with the new *augmentations* to see how the \"new\" data looks like..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a50b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_dls.show_batch(max_n=12, figsize=(8,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0395b945",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "...before doing the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db3f0a8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "aug_learn = vision_learner(aug_dls, mobilenet_v3_small, metrics=accuracy)\n",
    "aug_learn.fine_tune(epochs=2, base_lr=3e-3)  # quick test\n",
    "print('Accuracy:', aug_learn.validate()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f70ca6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "<font color='red'>TO-DO</font>: Do you get better results?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be303ea",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "## Different architecture\n",
    "\n",
    "<font color='red'>TO-DO</font>: Try a *larger* model, such as one from the *resnet* family (e.g., `resnet50`). You can stick with the original `DataBlock`. Is it worth it, considering the increase in training time?\n",
    "\n",
    "It should be possible to obtain an icomplete list of available models by using\n",
    "```\n",
    "import timm\n",
    "timm.list_models()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7511e7",
   "metadata": {},
   "source": [
    "# Sample questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253b7615",
   "metadata": {},
   "source": [
    "## What is the main goal of the image classification model described here?\n",
    "- [ ] To make image files smaller so they fit on disk\n",
    "- [ ] To decide which of two labels best matches a picture (for example, which kind of object it shows)\n",
    "- [ ] To turn all color pictures into black-and-white\n",
    "- [ ] To draw new pictures from scratch\n",
    "\n",
    "## Why are the images split into a training set and a validation set?\n",
    "- [ ] So the model can be trained on one part and then checked on images it has not seen\n",
    "- [ ] So that each image gets two different labels\n",
    "- [ ] So that half of the images can be safely deleted\n",
    "- [ ] So that the images can be sorted by file name"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
