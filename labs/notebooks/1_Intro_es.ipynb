{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7da65bfc-98a3-4345-85cc-9bcc711cdd9c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "> Introdución al procesado de lenguaje natural"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af63407d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "El Procesamiento de Lenguaje Natural (NLP, del inglés *Natural Language Processing*) sirve para enseñar a los ordenadores a entender y trabajar con el lenguaje humano (las palabras que usamos todos los días). Desde chatbots y aplicaciones de traducción hasta filtros de spam y asistentes de voz, el NLP impulsa muchas herramientas en las que confiamos sin siquiera notarlo. En este notebook, exploraremos algunas de las técnicas básicas que permiten a los ordenador manipular texto de manera eficaz."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ccd2c3",
   "metadata": {
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "# La tarea\n",
    "\n",
    "Sigue el notebook tratando de *entender* lo que estás haciendo. Por el camino, encontrarás algunos <font color='red'>TO-DO</font>s que plantean preguntas relacionadas con lo que acabas de hacer o te piden que escribas algún pequeño fragmento de código (Python) (ya sea para responder la pregunta o para continuar). Este notebook es solo una herramienta para que aprendas y **no** está pensado para ser entregado (de cara a la evaluación) una vez que termines.\n",
    "\n",
    "Fíjate que puedes pedir ayuda al *chatbot* integrado de *Colab* (*Gemini*) en cualquier momento. Está permitido, incluso recomendado (si uno es consciente de sus limitaciones y los posibles errores que puede cometer), pero ten en cuenta que en última instancia debes entender lo que está sucediendo (tener una \"visión general\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0797bf06",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b48a118",
   "metadata": {
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "# Curso rápido de notebooks [Jupyter](https://jupyter.org/)/[Colab](https://colab.research.google.com/?hl=en)\n",
    "\n",
    "Un notebook *jupyter*/*colab* tiene dos tipos de celdas:\n",
    "\n",
    "- *celdas de código*, pensadas para escribir y ejecutar código (diferentes lenguajes están soportados), y\n",
    "\n",
    "- *celdas markdown* para texto, imágenes, títulos, etc. (*esta* celda es una celda markdown).\n",
    "\n",
    "Casi todo se puede hacer con la interfaz (menús de arriba y botones junto a las celdas), pero usar atajos de teclado es muy práctico para ir rápido.\n",
    "\n",
    "## **Modos de Celda**\n",
    "- **Modo Edición**: Pulsa `Enter` para editar una celda.\n",
    "- **Modo Comando**: Pulsa `Esc` para interactuar con el notebook.\n",
    "\n",
    "## **Atajos de teclado (Modo Comando y Edición)**\n",
    "- **Ejecutar celda**: `Shift + Enter`\n",
    "- **Ejecutar celda e insertar debajo**: `Alt + Enter`\n",
    "\n",
    "## **Atajos (Modo Comando)**\n",
    "- **Insertar celda debajo**: `b`\n",
    "- **Insertar celda arriba**: `a`\n",
    "- **Borrar celda**: `d, d` (pulsa `d` dos veces)\n",
    "- **Cambiar a celda de código**: `y`\n",
    "- **Cambiar a celda markdown**: `m`\n",
    "\n",
    "## **Atajos (Modo Edición)**\n",
    "- **Autocompletar**: `Tab`\n",
    "\n",
    "*Google Colab* tiene integrado [Gemini](https://gemini.google.com) (la herramienta tipo ChatGPT de *Google*). Usando el botón *Gemini* al lado de una celda de código puedes pedir, por ejemplo, que te explique el código.\n",
    "\n",
    "Los notebooks tienen un estado interno (determinado por el código ya ejecutado), por lo que están **pensados para ejecutarse secuencialmente**. Normalmente empiezas por la primera celda y vas pulsando `Shift + Enter` para ejecutar la celda actual (después de leerla) y pasar a la siguiente.\n",
    "\n",
    "<font color='orange'>ADVERTENCIA</font>: Al ejecutar la primera *celda de código* del notebook verás una advertencia que dice que el notebook no fue creado por *Google*. Haz clic en \"Run anyway\" (no hay peligro: el código ni siquiera se ejecuta localmente).\n",
    "\n",
    "<font color='green'>SUGERENCIA</font>: Si seleccionas `Save a copy in Drive` en el menú `File` puedes guardar tu propia copia del notebook (con los cambios que hagas). Te pedirá que inicies sesión con tu cuenta *Google* (UC3M)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c6a5a1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d9b152",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "## Preparación del entorno\n",
    "\n",
    "Algunos `import`s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bd25cc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9201a16",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "A continuación usaremos dos recursos de [nltk](https://www.nltk.org) que no están instalados junto con la librería."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787ad19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f83d899",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "<font color='red'>TO-DO</font> ¿Cuáles son los dos recursos de NLTK que descargamos anteriormente?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39f1284",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "# Preprocesamiento de Texto\n",
    "\n",
    "El texto es inherentemente *datos no estructurados* en el sentido de que no sigue un formato fijo (no es una tabla, matriz o hoja de cálculo ordenada). Por lo tanto, se requiere cierto *preprocesamiento* antes de que el texto pueda ser alimentado a un modelo de aprendizaje automático (ML). Aprender a hacer este *preprocesamiento* es el propósito de esta sección.\n",
    "\n",
    "Observa que\n",
    "> Como los algoritmos de aprendizaje automático procesan números en lugar de texto, el texto debe convertirse a números.\n",
    ">\n",
    "> -- <cite>[Wikipedia](https://en.wikipedia.org/wiki/Large_language_model#Tokenization)</cite>\n",
    "\n",
    "### Tokenización\n",
    "\n",
    "La tokenización es el proceso de dividir un texto en trozo más pequeños, llamados tokens. Estos tokens pueden ser palabras, frases, o incluso oraciones. A cada token se le asigna entonces un número (único). La tokenización es un paso crucial en NLP ya que nos permite analizar y procesar datos textuales de manera más efectiva.\n",
    "\n",
    "ADVERTENCIA: Esto es notablemente diferente de la [tokenización *léxica*](https://en.wikipedia.org/wiki/Lexical_analysis#Lexical_token_and_lexical_tokenization), que no tiene interés para nosotros en este curso.\n",
    "\n",
    "Vamos a *tokenizar* el siguiente texto (puedes sustituirlo por cualquier texto que te guste)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22efb239",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'They can kill you, but the legalities of eating you are quite a bit dicier (from \"Infinite Jest\", by David Foster Wallace)'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28767e4a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "utilizando la librería `nltk`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5515baa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(text)\n",
    "print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbb4b09",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "Con este *tokenizador* concreto cada token es una palabra, y los signos de puntuación se tratan como tokens separados. Esto es útil para muchas tareas de NLP, pero hay otros tokenizadores que pueden manejar la puntuación de manera diferente o tokenizar a nivel de carácter.\n",
    "\n",
    "Después de la tokenización, simplemente numeras los tokens para que cada uno tenga un número asociado. Ese será su índice en la lista de posibles tokens que conforman el **vocabulario** del modelo, es decir, el conjunto de tokens posibles.\n",
    "\n",
    "<font color='red'>TO-DO</font> ¿Cuál sería aquí el vocabulario?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1cc850",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "### Eliminación de Stopwords\n",
    "\n",
    "Claramente algunas palabras son más informativas que otras. Si un niño pequeño dice \"gato árbol\", puedes adivinar que probablemente está tratando de decir \"hay un gato en el árbol\". Palabras como \"el\" o \"un\" son muy comunes en español, pero normalmente no llevan mucho significado por sí solas. Estas palabras comunes se llaman **stopwords**. Eliminar stopwords puede ayudar a reducir el *ruido* en los datos y mejorar el rendimiento de los modelos de NLP.\n",
    "\n",
    "Hay *listas predefinidas de stopwords* para explotar (¡¡descargamos una de esas listas arriba!!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcf3e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stop_words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24afdcd8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "Podríamos explotarlas para *filtrar* la lista anterior de `tokens`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56adb702",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_tokens = list(filter(lambda w: w.lower() not in stop_words, tokens))\n",
    "filtered_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92afb68a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "<font color='red'>TO-DO</font>: ¿Qué palabras fueron eliminadas como stopwords?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f944ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(tokens) - set(filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7338096c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "Más adelante veremos un ejemplo que muestra el efecto de eliminar stopwords.\n",
    "\n",
    "# Extracción de Características: Bag‑of‑Words y tf-idf\n",
    "\n",
    "En lugar de NLTK, aquí haremos uso de [scikit-learn](https://scikit-learn.org), una librería popular (casi podríamos decir *estándar*) para ML. Proporciona una serie de herramientas para procesamiento de texto, incluyendo métodos de extracción de características como Bag-of-Words y tf-idf (¡¡mantente atento!!).\n",
    "\n",
    "Creemos algunos *documentos*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6210e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    \"I love programming in Python.\",\n",
    "    \"Python is great for NLP tasks.\",\n",
    "    \"I enjoy learning new languages.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88295e5e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "¿Cómo podemos convertir cada documento en un vector de números de *tamaño fijo*? Una vez que tenemos un diccionario, podemos simplemente contar el número de veces que cada uno de sus elementos aparece en cada *documento*. Entonces, cada documento queda representado por un vector del tamaño del vocabulario. Esta *transformación* es una tarea muy común que *scikit-learn* automatiza a través de la clase [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html). Lo que resulta es una representación [Bag-of-words](https://en.wikipedia.org/wiki/Bag-of-words_model) (o *BoW*) del corpus (en la cual cada documento se convierte en un vector del tamaño del vocabulario).\n",
    "\n",
    "<font color='red'>TO-DO</font>: Utiliza la clase [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) (soporta *stopwords*) para *vectorizar* la `lista` de *documentos* anterior. Imprime el vector, es decir, los *conteos*, para el primer *documento*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf3e1b5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "<font color='red'>TO-DO</font>: ¿Cuál es el vocabulario? ¿Cuántas veces aparece la 3ª palabra del vocabulario en el 2º documento?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0606e7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "Ahora tenemos una manera de representar cada documento de texto en un corpus como un vector de números de tamaño fijo (*conteos*). Sin embargo, incluso después de descartar las *stopwords*, que tienen poco o ningún significado, está claro que no todas las palabras son igualmente significativas. Intuitivamente, las palabras que aparecen *todo el tiempo* en un corpus (por ejemplo, la palabra \"médico\" en una colección de documentos sobre medicina) no son muy significativas y, viceversa, las palabras que aparecen muy poco pueden proporcionar algunas pistas útiles para una tarea. Tenemos un nombre para esta intuición: [term frequency–inverse document frequency](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) o *tf-idf*. La idea es: para cada documento, contamos el número relativo de veces (*frecuencia*) que aparece cada palabra (*término*) y lo dividimos por el número relativo de veces que lo hace en el corpus (*frecuencia de documento*).\n",
    "\n",
    "<font color='red'>TO-DO</font>: Utiliza la clase [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) para obtener el tf-idf en lugar de conteos *crudos*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6011bbd9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "<font color='red'>TO-DO</font>: ¿Cuál es la palabra con los valores tf-idf no nulos más pequeños en todos los documentos?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353e36d0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "<font color='red'>TO-DO</font>:\n",
    "- ¿Cómo manejan la *dispersión* (cada documento solo incluye un pequeño número de términos del vocabulario) las clases anteriores (para evitar ocupar una cantidad enorme de memoria)?\n",
    "- Después de agregar un documento **sin** la palabra \"python\", ¿debería aumentar o disminuir el tf-idf de esta última en los documentos originales?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494cb92d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "## Por Qué Importan las Stopwords\n",
    "\n",
    "Observa que, una vez que cada documento se convierte en un vector de números de tamaño fijo (el del vocabulario, digamos $N$), es fácil comparar documentos comparando sus vectores correspondientes. En principio, cualquier *distancia* en $\\mathbb{R}^N$ es susceptible de ser utilizada.\n",
    "\n",
    "Calcularemos la [similitud coseno](https://en.wikipedia.org/wiki/Cosine_similarity) entre dos oraciones cortas **con** y **sin** eliminación de stopwords para ilustrar cómo las stopwords pueden inflar las puntuaciones de similitud. Un corpus ligeramente \"forzado\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9f69e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_sentence = \"The cat sat on the mat\"\n",
    "another_sentence = \"A dog sat on the rug\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f9e97f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "¿Son similares las oraciones?\n",
    "\n",
    "**Sin** eliminar stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a54149",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_all = CountVectorizer().fit_transform([a_sentence, another_sentence])\n",
    "print(\"Cosine similarity (without stopwords):\",\n",
    "      cosine_similarity(vec_all)[0,1].round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e912655b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "**Después** de eliminar las stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86f5029",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_ns = CountVectorizer(stop_words='english').fit_transform([a_sentence, another_sentence])\n",
    "print(\"Cosine similarity (with stopwords):\",\n",
    "      cosine_similarity(vec_ns)[0,1].round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9a96d5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "Observa cómo la eliminación de stopwords reduce la similitud al excluir palabras comunes como *the* y *on*, proporcionando una sensación más verdadera de distancia semántica.\n",
    "\n",
    "# Clasificación de Texto con Naive Bayes\n",
    "\n",
    "Pongamos en práctica lo que hemos aprendido construyendo un *clasificador de noticias*: dado una pieza de noticia, la tarea es decidir la *categoría* (tema, tópico) entre un conjunto de opciones. Usaremos solo **2 categorías**,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c1a57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['rec.autos', 'rec.sport.baseball']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82cb49e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    ", del dataset `20 Newsgroups`, que se puede descargar directamente a través de *scikit-learn*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de28541b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = fetch_20newsgroups(subset='train', categories=categories, remove=('headers','footers','quotes'))\n",
    "test  = fetch_20newsgroups(subset='test',  categories=categories, remove=('headers','footers','quotes'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67eda9f1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "<font color='red'>TO-DO</font>: Echa un vistazo a un par de *posts* (ya sea del conjunto de *entrenamiento* o *prueba*), junto con su *categoría* correspondiente (es decir, etiqueta)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed78047",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "<font color='red'>TO-DO</font>: Convierte los documentos en *conteos* tf-idf. Para ello, entrena el *vectorizador* utilizando el conjunto de entrenamiento y, después, usa el objeto `TfidfVectorizer` resultante para transformar tanto el conjunto de entrenamiento como el de prueba por separado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd9a5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf_vec = ...\n",
    "# X_train = ...\n",
    "# X_test = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c77753e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "<font color='red'>TO-DO</font>: Echa un vistazo al vector de números para los *posts* revisados anteriormente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809b6f18",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "<font color='red'>TO-DO</font>: ¿Qué pasa cuando hay una palabra en el conjunto de prueba que no fue vista en el conjunto de entrenamiento (cuando haces `fit_transform` en este último)? Inventa un documento con una sola palabra no existente y observa cuántos elementos no nulos hay en su representación tf-idf (obtenida a través del `*Vectorizer` ajustado en el conjunto de entrenamiento)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88425e03",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "Vamos a entrenar un [Naive Bayes classifier](https://en.wikipedia.org/wiki/Naive_Bayes_classifier) sobre el conjunto de *entrenamiento*..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb54ca9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, train.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec5f792",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "...y utilizarlo para predecir sobre el conjunto de *test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b59f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32155e10",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "<font color='red'>TO-DO</font>: ¿Cuál es la precisión general del clasificador? Necesitas comparar las predicciones anteriores contra los *targets* reales en el conjunto de *test*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7ad443",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "<font color='red'>TO-DO</font>: Revisa uno de los *posts* mal clasificados. ¿Eres capaz de decir a qué clase pertenece (es decir, de qué trata)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0334c864",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "<font color='red'>TO-DO</font>: ¿Cómo es que todos los textos terminan teniendo la misma longitud?\n",
    "\n",
    "# Introducción al Modelado de Tópicos\n",
    "\n",
    "El modelado de tópicos descubre tópicos latentes en un corpus *sin supervisión* (nótese que arriba teníamos *etiquetas*, es decir, supervisión, para cada noticia). Usaremos [Latent Dirichlet allocation](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation) o LDA (que no debe confundirse con *Linear discriminant analysis*, también abreviado en la literatura como LDA).\n",
    "\n",
    "Obtengamos más datos del conjunto de datos `20 Newsgroups`...pero esta vez sin hacer uso de las etiquetas (es decir, las categorías). De hecho, solo estamos usando el atributo `data` (**no** el `target`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be21e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['talk.politics.misc', 'comp.graphics', 'sci.space']\n",
    "data = fetch_20newsgroups(subset='train', categories=categories,remove=('headers','footers','quotes'))\n",
    "docs = data.data  # list of raw text documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fa81d8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "Primero *vectorizamos* el corpus usando un `CountVectorizer` básico, siendo este último más alineado con las suposiciones detrás de LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e092f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer(stop_words='english')\n",
    "X = vec.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd2cc02",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "Vamos a entrenar el modelo utilizando LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f7a9a4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components=3, random_state=42)\n",
    "lda.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfad1e1d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "Echemos un vistazo a las palabras más importantes (más probables) en cada tópico (es decir, `component`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe543fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = vec.get_feature_names_out()\n",
    "for idx, topic in enumerate(lda.components_):\n",
    "    top_terms = [vocab[i] for i in topic.argsort()[-5:][::-1]]\n",
    "    print(f\"Topic {idx+1}: {top_terms}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d66ed3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "Los tópicos anteriores, ¿encajan con las *categorías* en el conjunto de datos `20 Newsgroups`?\n",
    "\n",
    "<font color='red'>TO-DO</font>: Podría ser conveniente ignorar algunas de las palabras en los temas anteriores. Re-*ajusta* el modelo excluyendo un par de palabras: \"mr\" y \"don\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9a4b01",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "<font color='red'>TO-DO</font>: ¿Qué pasa cuando aumentas o disminuyes el número de temas? Fíjate que, ahora mismo, tenemos más tópicos que categorías *reales* en los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0d1492",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "<font color='red'>TO-DO</font>: ¿Puede una palabra aparecer en dos tópicos diferentes? ¿Qué sucede cuando modificas el valor de `random_state` pasado a `LatentDirichletAllocation`? ¿Por qué?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d55b23",
   "metadata": {},
   "source": [
    "# Sample questions\n",
    "\n",
    "## What happens if a word appears in the test set but not in the training vocabulary?\n",
    "- [x] It is ignored (given zero weight) by the vectorizer  \n",
    "- [ ] It causes an error  \n",
    "- [ ] The model guesses its meaning automatically  \n",
    "- [ ] The model adds it to the vocabulary dynamically  \n",
    "\n",
    "---\n",
    "\n",
    "## What is the main goal of topic modeling (e.g., LDA)?\n",
    "- [x] To discover hidden themes or topics in a collection of documents  \n",
    "- [ ] To classify documents into predefined categories  \n",
    "- [ ] To translate documents into another language  \n",
    "- [ ] To remove stopwords from long texts  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cei",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
