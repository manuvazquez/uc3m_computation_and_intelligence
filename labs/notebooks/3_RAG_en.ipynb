{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d28d1fa",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "> Large Language Models and Retrieval-Augmented Generation\n",
    "\n",
    "[Large Language Models](https://en.wikipedia.org/wiki/Large_language_model) (LLMs), powering tools such as *ChatGPT*m are great at *generating* text, but they don’t always know the latest information or specific facts we care about. This is where [Retrieval-Augmented Generation](https://en.wikipedia.org/wiki/Retrieval-augmented_generation) (RAG) comes in: instead of relying only on what the model \"remembers\", we can connect it to an external knowledge source (like documents, notes, or a database). With RAG, the model first retrieves the most relevant pieces of information, and then uses them to generate an accurate, helpful response. This approach makes LLMs more reliable, more up-to-date, and more useful for real-world applications like chatbots, assistants, and search systems.\n",
    "\n",
    "For (easily) putting together a RAG, we'll be making use of [LangChain](https://github.com/langchain-ai/langchain), which is an open-source framework that helps you quickly build LLM-powered apps and agents using prebuilt components and integrations.\n",
    "\n",
    "# Setup\n",
    "\n",
    "In principle, you could run the notebook either in Colab or locally. Is the notebook running in *Colab*?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e89d3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    running_in_colab = True\n",
    "except ImportError:\n",
    "    running_in_colab = False\n",
    "\n",
    "running_in_colab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2eea58",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "If in *Colab*, you might want to switch to GPU environment...or get comfy while the CPU is crunching numbers.\n",
    "\n",
    "If not running in *Colab*, you might want to choose a GPU if several are available. Ignore, if you are running in *Colab*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933e61b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not running_in_colab:\n",
    "\n",
    "    import os\n",
    "    os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757ef7c0",
   "metadata": {
    "tags": [
     "english"
    ]
   },
   "source": [
    "## Ollama\n",
    "\n",
    "We are going to exploit a freely available (trained by someone else) LLM. For *running* it, we will be using [Ollama](https://ollama.com/), which is software for\n",
    "\n",
    "> getting up and running large language models\n",
    "\n",
    "It relies on a client-server model, and hence we must start up the server before sending any request. \n",
    "\n",
    "- If in *Colab*, the required packages will be installed. You might see some ugly <font color='red'>ERROR/WARNING</font>s. Just ignore them. It should be fine.\n",
    "\n",
    "- If **not** in *Colab* you should install *ollama* on your own (along with every required *python* package)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52d3dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import pathlib\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "if running_in_colab:\n",
    "    !pip uninstall -y langchain 2>/dev/null || true # to avoid conflicts with new langchain version\n",
    "    !pip -q install -U langchain-core langchain-community langchain-text-splitters langchain-chroma langchain-huggingface langchain-ollama llama-index-core chromadb unstructured\n",
    "    !curl -fsSL https://ollama.com/install.sh | sh\n",
    "    !nohup ollama serve > /dev/null 2>&1 &\n",
    "\n",
    "else:\n",
    "\n",
    "    log_file = pathlib.Path('ollama.log').open('w')\n",
    "    proc = subprocess.Popen(\n",
    "        [str(shutil.which('ollama') or pathlib.Path.home() / 'ollama' / 'bin' / 'ollama'), 'serve'],\n",
    "        stdout=log_file,\n",
    "        stderr=subprocess.STDOUT,\n",
    "        start_new_session=True,\n",
    "    )\n",
    "    \n",
    "# give it some time to start\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d386f47",
   "metadata": {
    "tags": [
     "english"
    ]
   },
   "source": [
    "The LLM server (*ollama*) should be now up and running."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3424003",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "## Python libraries\n",
    "\n",
    "The remaining required `import`s are centralized here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e565ab39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# embedding\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "import ollama\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "\n",
    "# database\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "# model\n",
    "from langchain_ollama import OllamaLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca07478",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "# Data\n",
    "\n",
    "Let us download some data. Here we are using the short story [Bartleby, the Scrivener](https://en.wikipedia.org/wiki/Bartleby,_the_Scrivener) by Herman Melville (freely available at [Project Gutenberg](https://www.gutenberg.org/)), but you can use any *plain text* documents you like (just make sure to place them in the correct directory or modify the code accordingly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede6928a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "bartleby_url = 'https://www.gutenberg.org/ebooks/11231.txt.utf-8'\n",
    "response = requests.get(bartleby_url)\n",
    "response.raise_for_status()\n",
    "\n",
    "data_dir = pathlib.Path('data')\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "book_file = data_dir / 'bartleby.txt'\n",
    "\n",
    "with book_file.open('w', encoding='utf-8') as f:\n",
    "    f.write(response.text)\n",
    "\n",
    "print(f'Downloaded book to {book_file}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f012e606",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "It *loads* every document in directory `data` as a *LangChain* `Document` (a `list` with all the documents is returned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1fb7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = DirectoryLoader('data').load()\n",
    "type(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86292a5b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "<font color='red'>TO-DO</font>: How many documents do you have? Check the content of one of them. What's its type in Python? Try to access the text inside (the important attribute is `page_content`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0174defa",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "Each `Document` above must be turned into a **fixed-size** vector of real numbers. Before, we were achieving this using *Bag-of-words*, but different (already trained) models can be summoned for this task, and here we'll be using a *sentence transformer* (never mind for now). This is a much more sophisticated model (preserving semantic and contextual information) than *Bag-of-words*. The resulting **fixed-size** vectors of real numbers are often referred to as [embeddings](https://en.wikipedia.org/wiki/Embedding_(machine_learning))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0864ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "embed_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ff0561",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "<font color='red'>TO-DO</font>: Embed any sentence you like (just call the above `embed_model` on any text), and then another that is really close. Check their corresponding vectors and compare them somehow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7636a7e2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "<font color='red'>TO-DO</font>: Try (a few) [different models](https://huggingface.co/sentence-transformers/models) to carry out the \n",
    "embedding of the `Document`s. Do you see any difference (speed, performance...)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b3c4f3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "\n",
    "The number of *tokens* fed as input to a sentence transformer is limited (that was not the case for *Bag-of-words*). The documentation for the [all-MiniLM-L6-v2]((https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) ) model used above states\n",
    "\n",
    "> input text longer than 256 word pieces is truncated.\n",
    "\n",
    "In order to deal with this, documents are split into **overlapping** sequences of (here) 500 *characters* (a *token* encompasses several of them). This splitting of the documents is also a good thing for having a finer *granularity* when looking for information in the document since the RAG can locate and return the specific segment where relevant information appears instead of an entire long document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9789e425",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "documents = splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c385fa3c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "<font color='red'>TO-DO</font>: After splitting, how many documents do you have? What are the lengths of the first three?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8b99b3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "# Storing the documents\n",
    "\n",
    "A [Chroma](https://www.trychroma.com/) database is assembled to store the *embedded documents*. When instantiating the corresponding object, we need to pass in the embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5fa118",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma.from_documents(\n",
    "    documents, embedding=embed_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70067887",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "Notice the above [`from_documents`](https://reference.langchain.com/python/integrations/langchain_chroma/#langchain_chroma.Chroma.from_documents) method embeds and stores the documents in one go.\n",
    "\n",
    "<font color='red'>TO-DO</font>: What kind of database is [Chroma](https://www.trychroma.com/)? How does it differ from traditional databases?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6177ef",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "You can exploit the [Chroma](https://www.trychroma.com/) database to look for sentences *similar* to a given one. Every search in the database might (will, in principle) return several hits (as many as requested through the parameter`k`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d01befc",
   "metadata": {},
   "outputs": [],
   "source": [
    "hits = vectordb.similarity_search(\n",
    "    \"I would prefer not to do it\", k=3\n",
    ")\n",
    "len(hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41635af",
   "metadata": {},
   "outputs": [],
   "source": [
    "for h in hits:\n",
    "    print(h.page_content[:120], '\\n---')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f848100",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "<font color='red'>TO-DO</font>: Search for:\n",
    "\n",
    "- A sentence that is close to something that you know is there (in the text).\n",
    "\n",
    "- A sentence that is (most likely) not there at all."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a78acc6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "# LLM\n",
    "\n",
    "## Model\n",
    "\n",
    "Let us go and pick a model. You can check [here](https://ollama.com/library?sort=popular) for which ones are freely available. Let us start with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c0bf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = 'llama3:8b'\n",
    "# model_name = 'deepseek-r1:8b'\n",
    "model_name = 'llama3.2:1b' # faster download"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d94a30",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "CAVEAT: some models will also report the thinking process (via `<think>` tags), which might not be very convenient for some tasks, and we might want to filter out.\n",
    "\n",
    "A model must be downloaded only once (...per session if you are using *Colab*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581bd726",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama.pull(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66269e7b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "Once the server is running, a (previously downloaded) model can be accessed through a `langchain` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb35c4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OllamaLLM(model=model_name, temperature=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055806df",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "<font color='red'>TO-DO</font>: Ask the LLM model whatever (using the `invoke` *method*), e.g., \"What is backpropagation?\" or \"What is Rick and Morty?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdba5b5b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "<font color='red'>TO-DO</font>: Ask the LLM model about the main character in the novel, Bartleby. We are doing this **before** *augmenting* what the LLM knows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe64fdf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "## Templates\n",
    "\n",
    "If you plan to interact with an LLM using always a certain style of *prompting*, you can make a template for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9e0e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt = ChatPromptTemplate([\n",
    "    ('system', 'Try to rhyme every answer you give.'),\n",
    "    ('human', '{user_input}'),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a1fcea",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "Then you build a *chain* (to be invoked just like we did above) that *sends* (read `|` as $\\to$) the template to the LLM and the result of the latter to a *parser* that outputs a `str`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0f274e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = chat_prompt | llm | StrOutputParser()\n",
    "print(chain.invoke('Explain backpropagation in one paragraph.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdb81a3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "<font color='red'>TO-DO</font>: Experiment with the above `system` instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b29bfce",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "<font color='red'>TO-DO</font>: Make a new *chain* (stored in a new variable) that *keeps only the last word in every sentence and gets rid of the rest*. For the sake of simplicity (we don't need `system` instructions), you might want to use `PromptTemplate.from_template`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1612f281",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "<font color='red'>TO-DO</font>: Make yet another chain by *piping* (`|`) the two previous ones in order to get a chain that yields the *rhyming* words in the above output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfa1631",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "# Retriever + LLM\n",
    "`langchain` provides a way to construct an entire end-to-end RAG. We just need:\n",
    "\n",
    "- an LLM, \n",
    "\n",
    "- a database of documents (which is tidied up using `as_retriever` so that `langchain` knows how to \"talk\" to it), and\n",
    "\n",
    "- a *prompt* factoring in the latter.\n",
    "\n",
    "Let us start with the *prompt*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb746993",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = (\n",
    "    'Use the given context to answer the question. '\n",
    "    'If you don\\'t know, say so. '\n",
    "    'Keep the answer under three sentences.\\n\\n'\n",
    "    'Context:\\n{context}'\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [('system', system_prompt), ('human', '{input}')]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cd2b85",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "A function simply joining the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6e4304",
   "metadata": {},
   "outputs": [],
   "source": [
    "join_docs = RunnableLambda(\n",
    "    lambda docs: '\\n\\n'.join(d.page_content for d in docs)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c573382",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "`context` and `input` are the inputs to (*fill in*) the `prompt`, which is sent to the `llm`, whose output is *parsed* to yield the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdaeeaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "piped_rag_chain = (\n",
    "    {                                             #  a `dict` for the prompt\n",
    "        'context': vectordb.as_retriever(search_kwargs={\"k\": 4}) | join_docs,\n",
    "        'input':   RunnablePassthrough(),\n",
    "    }\n",
    "    | prompt                                      # fill {context} + {input}\n",
    "    | llm                                         # generate answer\n",
    "    | StrOutputParser()                           # ChatMessage → str\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8dd130",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "<!-- <font color='red'>TO-DO</font>: Ask this *chain* (using the `invoke` *method*) again about *backpropagation*. Compare answers. -->\n",
    "<font color='red'>TO-DO</font>: Ask this *chain* (using the `invoke` *method*) again the same questions you asked above (Bartleby, and whatever you asked before that). Compare answers (now **after** *augmenting* the LLM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a6553d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not running_in_colab:\n",
    "    proc.terminate()   # or proc.kill() for a hard stop\n",
    "    proc.wait()\n",
    "# log.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea396b5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "<font color='red'>TO-DO</font> : Try a different language model (you have a couple commented out in the code setting `model_name`). Be careful with the size of the model (*7b*, *8b*...). Large models might take **a lot** of time to produce an answer...or even collapse the GPU memory. The suffix *b* in those models stands for *billion*. The *Toy GPT* we trained before had...0.2 **m**illion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa317933",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "english"
    ]
   },
   "source": [
    "# Sample questions\n",
    "\n",
    "## What is the main benefit of adding retrieval to an LLM-based system?\n",
    "- [ ] It removes the need to design prompts.\n",
    "- [ ] It brings in relevant external context first, making answers more accurate and up-to-date.\n",
    "- [ ] It permanently rewrites the model’s weights with new facts.\n",
    "- [ ] It forces the model to answer only with quotes.\n",
    "\n",
    "## What does an embedding represent for a piece of text?\n",
    "- [ ] A perfect, lossless compression of the original text.\n",
    "- [ ] The count of words and punctuation only.\n",
    "- [ ] A numeric vector that captures meaning so similar texts are close together.\n",
    "- [ ] A list of keywords in alphabetical order."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cei",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
