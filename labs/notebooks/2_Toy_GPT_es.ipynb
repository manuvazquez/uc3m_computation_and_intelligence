{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "> ChatGPT *de juguete*\n",
    "\n",
    "Vamos construir un *ChatGPT* \"mini\" (lo suficientemente pequeño como para entenderlo en unos minutos, pero lo suficientemente real como para ver la *magia* de la predicción de texto). En este notebook seguiremos el enfoque de [Andrej Karpathy](https://karpathy.ai/): escribir el código desde cero, hacerlo de manera muy simple, y entrenar un pequeño modelo que pueda aprender a generar texto carácter a carácter. No estamos buscando potencia o velocidad. El objetivo es desmitificar cómo funcionan realmente estos modelos por dentro. Al final, tendrás una comprensión práctica de cómo se puede construir un sistema tipo GPT paso a paso, y podrás jugar con él y experimentar.\n",
    "\n",
    "Este *notebook* está basado en los repositorios [ng-video-lecture](https://github.com/karpathy/ng-video-lecture/tree/master) y [minGPT](https://github.com/karpathy/minGPT) de Karpathy, y el [video tutorial complementario](https://www.youtube.com/watch?v=kCc8FmEb1nY).\n",
    "\n",
    "# Preparación del entorno\n",
    "\n",
    "En principio, podrías ejecutar el *notebook* en *Colab* o localmente. ¿El *notebook* se está ejecutando en *Colab*?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    running_in_colab = True\n",
    "except ImportError:\n",
    "    running_in_colab = False\n",
    "\n",
    "running_in_colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "## GPU\n",
    "\n",
    "Para ejecutar este *notebook* (en un tiempo razonable), utilizaremos la [unidad de procesamiento gráfico](https://en.wikipedia.org/wiki/Graphics_processing_unit) (GPU) que proporciona el entorno *Colab*. Para habilitarla, en la parte superior derecha de la interfaz de *Colab*, haz clic en `Conectar`, `Cambiar tipo de entorno de ejecución`, selecciona `GPU T4` y, a continuación, haz clic en `Guardar`. Las llamadas `to_device` que aparecen dispersas por todo el *notebook* tienen por objeto *mover* las matrices a la GPU (si hay una disponible).\n",
    "\n",
    "Si no se ejecuta en *Colab*, es posible que quieras elegir una GPU si hay varias disponibles. Ignora esto si se ejecuta en *Colab*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not running_in_colab:\n",
    "\n",
    "    import os\n",
    "    os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "## Bibliotecas de Python\n",
    "\n",
    "Algunos `import`s necesarios se \"centralizan\" aquí."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "Hay una GPU disponible?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "# Preparación de datos\n",
    "\n",
    "Vamos a descargar un texto. El código de abajo descargará los textos de Shakespeare, pero puedes usar esencialmente cualquier recurso de texto (un libro, alguna página web...) que te guste!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "id": "h5hjCcLDr2WC",
    "outputId": "43f10117-d670-438f-fc0b-c6a0b7d64a3a",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se lee en memoria el archivo *completo*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "O6medjfRsLD9",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "print('number of characters read: ', len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "Vamos a echar un vistazo a nuestro conjunto de datos.\n",
    "\n",
    "<font color='red'>TO-DO</font>: Muestra los primeros 200 caracteres."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "## Vocabulario\n",
    "\n",
    "Como es habitual, para convertir texto en números necesitamos un **vocabulario** que nos permita construir, *pieza* a *pieza*, todo el dataset. Por simplicidad, vamos a considerar los caracteres individuales que aparecen en el texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "id": "0e-Rbyr8sfM8",
    "outputId": "5e2a5946-9664-4c0a-cbe8-236ca8b28d36",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "chars = sorted(set(text))\n",
    "vocab_size = len(chars)\n",
    "print(rf'Vocabulary ({len(chars)} elements) is: {''.join(chars)} ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "Vamos a asociar un índice (`i`) a cada elemento (carácter, `s`) del vocabulario. Cualquier *mapeo* nos sirve, así que lo más fácil es asociar cada carácter con su índice en la lista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yw1LKNCgwjj1",
    "outputId": "c02c56da-160a-41ef-8c16-819e18d5aec8"
   },
   "outputs": [],
   "source": [
    "stoi = { ch:i for i, ch in enumerate(chars) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "(`stoi` del inglés *string to integer*)\n",
    "\n",
    "Puedes utilizarlo para averiguar el índice de cualquier carácter que quieras, p. ej.,\n",
    "\n",
    "<font color='red'>TO-DO</font>: ¿Cuál es el índice del carácter `k`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "También necesitamos el *mapeo* inverso, es decir, de índice a carácter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itos = chars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "<font color='red'>TO-DO</font>: ¿Cuál es el carácter asociado con el índice `12`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "Aprovechamos los *mapeos* anteriores (un `dict` y una `list`, respectivamente) para hacer funciones capaces de operar, respectivamente, sobre **secuencias** de caracteres (para la *codificación*)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "print(encode(\"hii there\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "...y números (para la *decodificación*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "print(decode(encode(\"hii there\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "Vamos a codificar (caracteres a números) el conjunto de datos en un `Tensor` de *PyTorch*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YJb0OXPwzvqg",
    "outputId": "673e43ab-31a5-4758-defa-b1198285ab27"
   },
   "outputs": [],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "Tiene el mismo número de elementos que `text` arriba, y cada uno de los elementos es un entero de 64 bits (`torch.int64`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(text) == len(data)\n",
    "print(data.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "Imprimimos los primeros caracteres, ahora representados como números"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "## Partición training/validation\n",
    "\n",
    "Los datos se dividen en conjuntos de *training* y *validation*, de modo que tengamos una forma de saber como de bien está generalizando el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f_WIXqxz0lU5"
   },
   "outputs": [],
   "source": [
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "## Tamaño de bloque\n",
    "\n",
    "Como no podemos procesar *todos* los datos de una vez (a menos que tengas un dataset muy pequeño, lo cual probablemente acabaría dando lugar a un modelo muy malo), necesitamos *fragmentarlos*. Consideremos fragmentos de tamaño"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TD5Bj8Y6IAD4",
    "outputId": "738485b8-30a8-4d15-d265-39ca4ba7d509"
   },
   "outputs": [],
   "source": [
    "block_size = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "[GPT-4](https://en.wikipedia.org/wiki/GPT-4), por ejemplo, tiene un tamaño de bloque (también conocido como *context length*) de decenas de miles de *tokens*, cada uno de ellos abarcando más de un carácter. Por tanto, ten en cuenta que estamos, por supuesto, mirando un ejemplo de juguete\n",
    "\n",
    "Echemos un vistazo al primer bloque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[:block_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "Al procesar cada bloque, el objetivo es predecir un carácter dados *todos* los anteriores: para predecir el 2º carácter solo usaremos el 1º, al predecir el 3º carácter, usaremos el 1º y 2º...y así sucesivamente. En principio, esto significaría que, para cada tamaño de bloque, estaríamos haciendo `block_size - 1` predicciones. Siempre se considera un carácter extra, el carácter `(block_size+1)`-ésimo, de modo que tengamos exactamente `block_size` predicciones. Por tanto, el bloque anterior producirá las siguientes tareas de predicción:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9HXDe8vGJCEn",
    "outputId": "66670795-362e-48ec-f681-1e2d3b783fd5"
   },
   "outputs": [],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f'When input is {context.tolist()}, the target: {target}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "En última instancia, esto es un problema de clasificación *multiclase*: cada predicción no es solo una etiqueta, sino una **distribución de probabilidad completa** sobre todas las clases posibles. En nuestro caso, refleja qué probabilidad tiene cada carácter del vocabulario de ser el siguiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "## Batching\n",
    "Como queremos aprovechar al máximo las GPUs (procesamiento paralelo), *empaquetaremos* y procesaremos varios bloques al mismo tiempo...tantos como"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "Vamos a fijar la semilla del generador de números pseudo-aleatorios (PRNG) para que obtengamos siempre los mismos resultados (al generalos números \"aleatorios\" más abajo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q3k1Czf7LuA9",
    "outputId": "e8a938ff-19d3-4ff2-fa1b-4bd2b6a7a1af"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "Una función auxiliar para ensamblar un batch aleatorio, ya sea del conjunto de *training* o del de *validation* (dependiendo del valor del parámetro `split`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split: str):\n",
    "    \n",
    "    # either the training or validation set\n",
    "    data = train_data if split == 'train' else val_data\n",
    "\n",
    "    # a random index (for each block in the batch) that is followed by at least `block_size` characters so that we can extract a full block\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    \n",
    "    # notice the `stack`ing\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "Vamos a generar un batch del conjunto de *training*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb, yb = get_batch('train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "...y echemos un vistazo a las entradas y salidas.\n",
    "\n",
    "- **Input** (entrada) y sus dimensiones (`bath_size`, `block_size`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xb)\n",
    "print(xb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "- **Target** (salida) y sus dimensiones (`bath_size`, `block_size`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(yb)\n",
    "print(yb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "<font color='red'>TO-DO</font>: muestra el *texto* representado por `yb` (las salidas o targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "Observa que"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb.shape == yb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "El batch anterior plantea los siguientes problemas de predicción:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for every sequence in the batch...\n",
    "for i_b, b in enumerate(range(batch_size)):\n",
    "\n",
    "    print(f'{i_b}-th element in the batch:')\n",
    "    \n",
    "    # for every element in the sequence...\n",
    "    for t in range(block_size):\n",
    "        \n",
    "        # every character in the sequence up to and including (hence the `+1`) t\n",
    "        context = xb[b, :t+1]\n",
    "        \n",
    "        # by construction (above), `yb[b,t]` is the target for the sequence up to and including t\n",
    "        target = yb[b,t]\n",
    "        \n",
    "        print(f\"When input is {context.tolist()}, the target: {target}\")\n",
    "\n",
    "    print('-'*5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "Lo que debe entrar en la red neuronal (NN) son en realidad `tensor`es y **no** `list`s de tamaño *variable*. La entrada a la NN será"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qpyyAeIzQjlO",
    "outputId": "59e7424f-1267-4ecd-bd24-c68d52291bda"
   },
   "outputs": [],
   "source": [
    "xb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "y la salida correspondiente (*target*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "Esto da lugar a `bath_size` $\\times$ `block_size` (las dimensiones de `xb` e `yb`) predicciones *independientes* para que el modelo aprenda ([explicación de Karpathy](https://youtu.be/kCc8FmEb1nY?t=1281)). Todas se procesan simultáneamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "# Entrenamiento\n",
    "\n",
    "## Parámetros\n",
    "\n",
    "Configuramos algunos (hiper)parámetros que utilizamos durante el entrenamiento del modelo\n",
    "\n",
    "- Vistos arriba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hoelkOrFY8bN",
    "outputId": "13811e1e-29c7-4794-c8d3-0a783c607aaa"
   },
   "outputs": [],
   "source": [
    "block_size = 32\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "- ¿Cuántos batches (aleatorios) para entrenar el modelo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_iters = 5000\n",
    "max_iters = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "- Algunos parámetros específicos de la arquitectura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "* En relación con el entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "## Modelo\n",
    "\n",
    "Esta es la definición de la NN (basada en [transformers](https://es.wikipedia.org/wiki/Transformador_(modelo_de_aprendizaje_autom%C3%A1tico))). **Ignórala** por ahora: en este curso todavía no nos interesan los detalles de implementación, aunque, como puedes ver, el código no es realmente grande. Si profundizas en el código (de nuevo, no es necesario), ten en cuenta que esto se escribió con una mentalidad educativa, y hay algunas prácticas de programación muy cuestionables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "# Renamed from https://github.com/karpathy/ng-video-lecture/blob/52201428ed7b46804849dea0b3ccf0de9df1a5c3/bigram.py#L61\n",
    "class ToyLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "Todo lo anterior solo sirve para definir una función (enorme), instanciada como"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ToyLanguageModel().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "Evalúemos `model` sobre el batch anterior, `xb` (no fue generado con los parámetros que estamos considerando ahora, pero esto no es un problema)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yb_est = model(xb.to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "<font color='red'>TO-DO</font>: ¿Qué devuelve? Explica las dimensiones de cada `Tensor`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "## Bucle de entrenamiento\n",
    "\n",
    "Una función para estimar la *función de pérdida*, una medida de \"lo bien que lo estamos haciendo\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X.to(device), Y.to(device))\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "Código \"genérico\" para entrenar un modelo. Llegado este punto, esto ya es bastante \"comprensible\" para ti. En cualquier caso, céntrate simplemente en saber \"qué está pasando\" a alto nivel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ToyLanguageModel()\n",
    "\n",
    "m = model.to(device)\n",
    "\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb.to(device), yb.to(device))\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "<font color='red'>TO-DO</font>: ¿Está garantizado que has entrenado sobre el dataset *completo* (es decir, que se ha usado cada carácter)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "<font color='red'>TO-DO</font>: Echa un vistazo a las dimensiones de `logits` (de la última iteración en el bucle de entrenamiento), y explícalas. Los `logits` vienen a ser probabilidades antes de ser *normalizadas* para que sumen $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "Para utilizar el modelo entrenado para generar texto nuevo, necesitamos configurar un contexto (una especie de punto de partida)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "<font color='red'>TO-DO</font>: ¿Cuál es el texto asociado con el contexto?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "Generemos texto (hasta 2.000 caracteres) usando el contexto anterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "# Experimentos\n",
    "\n",
    "- <font color='red'>TO-DO</font>: Entrena el modelo durante menos tiempo (es decir, sobre un número menor de batches, digamos 10), e intenta generar texto. ¿Qué observas? ¿qué diferencia hay entre la *función de pérdida* al final del entrenamiento con la del primer modelo que entrenaste? Entrena el modelo durante más tiempo y responde otra vez a las preguntas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "- <font color='red'>TO-DO</font>: Prueba diferentes tamaños de bloque. ¿Puedes conseguir mejores resultados?\n",
    "\n",
    "- <font color='red'>TO-DO</font>: Prueba con un dataset diferente (al de Shakespeare) más pequeño. Podrías, por ejemplo, poner la letra de tu canción favorita (un dataset muy pequeño) en la variable `text` de arriba. ¿Qué observas? Compara los valores de la *función de pérdida* para *training* y *validation*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "spanish"
    ]
   },
   "source": [
    "# Preguntas de ejemplo\n",
    "\n",
    "## ¿Qué controla el tamaño del *batch* durante el entrenamiento?\n",
    "- [ ] Cuántas capas tiene el modelo  \n",
    "- [ ] El número de épocas de entrenamiento\n",
    "- [ ] Cuántos trozos pequeños de datos se procesan en paralelo antes de actualizar los parámetros del modelo\n",
    "- [ ] El número máximo de caracteres que se generan\n",
    "\n",
    "---\n",
    "\n",
    "## ¿Qué está intentando aprender el modelo durante el entrenamiento?\n",
    "- [ ] El significado de palabras y frases  \n",
    "- [ ] Las reglas gramaticales del inglés  \n",
    "- [ ] El tono emocional de las obras de Shakespeare\n",
    "- [ ] La probabilidad del siguiente carácter dado los anteriores"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "cei",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
